{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc6c91c-de97-4491-98a4-25202da67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d635-95ad-4fd8-84e7-29a40503a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88648d7-c858-4786-b49f-24068483d4ab",
   "metadata": {},
   "source": [
    "#### First exercize\n",
    "It calculates average definition lenght for each section (nouns, verbs, adjectives and adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae19b6e-e10a-4b85-97c3-8cec8e5378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def avg_len_section_definitons():\n",
    "    pos_tag_list = ['n', 'v', 'a', 'r']\n",
    "    average_lenghts = []\n",
    "\n",
    "    for pos_tag in pos_tag_list:\n",
    "        synsets_lenght = []\n",
    "        for synset in list(wn.all_synsets(pos_tag)):\n",
    "            synsets_lenght.append(len(synset.definition().split(\" \")))\n",
    "        average_lenghts.append((pos_tag, mean(synsets_lenght)))\n",
    "\n",
    "    print(\"\\n\", average_lenghts, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a398d-e50c-41be-8fd5-1129d23a25ba",
   "metadata": {},
   "source": [
    "#### Second exercize\n",
    "The variation of the length along the path of the hyperonyms that lead from a given synset to its root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdd85c8-d997-466a-94c5-d1cb641669fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word):\n",
    "    \n",
    "    def_lens = []\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "\n",
    "        single_path = []\n",
    "        \n",
    "        hyp_path = syn.hypernym_paths()\n",
    "        \n",
    "        for i in range (0, len(hyp_path[0])):\n",
    "            \n",
    "            single_path.append((hyp_path[0][i],len((hyp_path[0][i].definition()).split())))\n",
    "\n",
    "        def_lens.append(single_path)\n",
    "        \n",
    "    return def_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb97c5f-143d-4c46-af14-cdf072f35c26",
   "metadata": {},
   "source": [
    "#### Third exercize\n",
    "Distance from the word's root and words within the definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c60b5d8-8368-4c47-99dd-a33b349eff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_root(synset):\n",
    "    return (min([len(path) for path in synset.hypernym_paths()]))\n",
    "\n",
    "def distance_root(word):\n",
    "    \n",
    "    output = dict()\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        actual_syn_dis = calculate_distance_root(syn)\n",
    "        output[syn] = {word :actual_syn_dis} \n",
    "                \n",
    "        syn_definition_processed = pre_processing(syn.definition())\n",
    "        for def_word in syn_definition_processed:\n",
    "            for def_syn in wn.synsets(def_word):\n",
    "                output[syn].update({def_word : calculate_distance_root(def_syn)})\n",
    "                        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a7afe-0b0b-4ab8-903e-63839a16a3f0",
   "metadata": {},
   "source": [
    "#### Fourth exercize\n",
    "Calculate bleu and rouges score between hypernyms and hyponyms of the words definition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bcb3da-320a-46bf-904d-1616f7b34952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bleu import multi_list_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def definition_overlap(word):\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "        \n",
    "        actual_def_processed = syn.definition()\n",
    "        \n",
    "        print (\"\\n--------------------\\nDefinition of\", syn,  \"=\", actual_def_processed)\n",
    "        print ()\n",
    "        \n",
    "        hyper_list = syn.hypernyms()\n",
    "        \n",
    "        for hy in hyper_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hyper_list) != 0):\n",
    "            print (\"Bleu score for hypernyms (1-gram):\", bleu_count / len(hyper_list))\n",
    "            print (\"Rogue f1 for hypernyms (1-gram):\", f_count / len(hyper_list))\n",
    "        else:\n",
    "            print(\"No hypernyms\")\n",
    "\n",
    "               \n",
    "        print ()\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "\n",
    "        hypo_list = syn.hyponyms()\n",
    "               \n",
    "        for hy in hypo_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hypo_list) != 0):\n",
    "            print (\"Bleu score for hyponyms (1-gram):\", bleu_count / len(hypo_list))\n",
    "            print (\"Rogue f1 for hyponyms (1-gram):\", f_count / len(hypo_list))\n",
    "        else:\n",
    "            print(\"No hyponyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156e046d-1568-48a8-b0a1-24c50baeff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [('n', 11.470035925226815), ('v', 6.146655044672042), ('a', 7.238433575677462), ('r', 5.028169014084507)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_len_section_definitons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb7ef8b-82e9-4774-b7b4-83ffb7bf5067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('matter.n.03'), 7),\n",
       "  (Synset('substance.n.01'), 11),\n",
       "  (Synset('material.n.01'), 12),\n",
       "  (Synset('paper.n.01'), 15)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('communication.n.02'), 12),\n",
       "  (Synset('written_communication.n.01'), 10),\n",
       "  (Synset('writing.n.02'), 24),\n",
       "  (Synset('essay.n.01'), 6),\n",
       "  (Synset('composition.n.08'), 8)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('instrumentality.n.03'), 13),\n",
       "  (Synset('medium.n.01'), 9),\n",
       "  (Synset('print_media.n.01'), 6),\n",
       "  (Synset('press.n.02'), 16),\n",
       "  (Synset('newspaper.n.01'), 14)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('instrumentality.n.03'), 13),\n",
       "  (Synset('medium.n.01'), 9),\n",
       "  (Synset('paper.n.04'), 5)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('communication.n.02'), 12),\n",
       "  (Synset('expressive_style.n.01'), 25),\n",
       "  (Synset('writing_style.n.01'), 7),\n",
       "  (Synset('prose.n.01'), 6),\n",
       "  (Synset('nonfiction.n.01'), 6),\n",
       "  (Synset('article.n.01'), 9),\n",
       "  (Synset('paper.n.05'), 11)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('group.n.01'), 9),\n",
       "  (Synset('social_group.n.01'), 5),\n",
       "  (Synset('organization.n.01'), 7),\n",
       "  (Synset('enterprise.n.02'), 6),\n",
       "  (Synset('business.n.01'), 11),\n",
       "  (Synset('firm.n.01'), 14),\n",
       "  (Synset('publisher.n.01'), 6),\n",
       "  (Synset('newspaper.n.02'), 6)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('creation.n.02'), 10),\n",
       "  (Synset('product.n.02'), 11),\n",
       "  (Synset('newspaper.n.03'), 11)],\n",
       " [(Synset('cover.v.01'), 9), (Synset('paper.v.01'), 3)],\n",
       " [(Synset('cover.v.01'), 9), (Synset('wallpaper.v.01'), 3)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypernym_paths(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332db380-cb06-4da6-bdfd-29fd9080073c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-91b2441b475e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdistance_root\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"courage\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-8b4aa554f41d>\u001b[0m in \u001b[0;36mdistance_root\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msyn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mactual_syn_dis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msyn_definition_processed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msyn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdef_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msyn_definition_processed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdef_syn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdef_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cefd0f600a54>\u001b[0m in \u001b[0;36mpre_processing\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Remove punctuation from a list of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-cefd0f600a54>\u001b[0m in \u001b[0;36mremove_punctuation\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Remove punctuation from a list of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[^\\w\\s]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Remove stopwords from a list of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "distance_root(\"courage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b0e43-226e-49c3-b8a6-fae5e1ee8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_overlap(\"courage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
