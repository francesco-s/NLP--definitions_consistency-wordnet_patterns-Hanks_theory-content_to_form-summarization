{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc6c91c-de97-4491-98a4-25202da67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d635-95ad-4fd8-84e7-29a40503a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(remove_punctuation(tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20f04b2-3273-49ef-a190-04c7b249c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words_list):\n",
    "    stopwords_list = get_stopwords()\n",
    "    new_words_list = []\n",
    "    for word in words_list:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stopwords_list:\n",
    "            new_words_list.append(word_lower)\n",
    "    return new_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38081c66-6579-4229-8ac6-728ae2e59933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words_list):\n",
    "    new_words_list = []\n",
    "    for word in words_list:\n",
    "        temp = word\n",
    "        if not temp.strip(string.punctuation) == \"\":\n",
    "            new_word = word.lower()\n",
    "            new_word = new_word.replace(\"'\", \"\")\n",
    "            new_words_list.append(new_word)\n",
    "    return new_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fe0474-8a95-451b-9993-373cc9196848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc570cd4-3026-4c1e-b8c1-116cbe901e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words_list):\n",
    "    \n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    \n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fdd85c8-d997-466a-94c5-d1cb641669fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word):\n",
    "    \n",
    "    def_lens = []\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "\n",
    "        single_path = []\n",
    "\n",
    "        hyp_path = syn.hypernym_paths()\n",
    "\n",
    "        for i in range (0, len(hyp_path[0])):\n",
    "            single_path.append(len((hyp_path[0][i].definition()).split()))\n",
    "\n",
    "        def_lens.append(single_path)\n",
    "        \n",
    "    return def_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c60b5d8-8368-4c47-99dd-a33b349eff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_root(synset):\n",
    "    return (min([len(path) for path in synset.hypernym_paths()]))\n",
    "\n",
    "def distance_root(word):\n",
    "    \n",
    "    output = dict()\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        actual_syn_dis = calculate_distance_root(syn)\n",
    "        output[syn] = {word :actual_syn_dis} \n",
    "                \n",
    "        syn_definition_processed = pre_processing(syn.definition())\n",
    "        for def_word in syn_definition_processed:\n",
    "            for def_syn in wn.synsets(def_word):\n",
    "                output[syn].update({def_word : calculate_distance_root(def_syn)})\n",
    "                        \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bcb3da-320a-46bf-904d-1616f7b34952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from bleu import multi_list_bleu\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def definition_overlap(word):\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "        \n",
    "        actual_def_processed = syn.definition()\n",
    "        \n",
    "        print (\"\\n--------------------\\nDefinition of\", syn,  \"=\", actual_def_processed)\n",
    "        print ()\n",
    "        \n",
    "        hyper_list = syn.hypernyms()\n",
    "        \n",
    "        for hy in hyper_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hyper_list) != 0):\n",
    "            print (\"Bleu score for hypernyms (1-gram):\", bleu_count / len(hyper_list))\n",
    "            print (\"Rogue f1 for hypernyms (1-gram):\", f_count / len(hyper_list))\n",
    "        else:\n",
    "            print(\"No hypernyms\")\n",
    "\n",
    "               \n",
    "        print ()\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "\n",
    "        hypo_list = syn.hyponyms()\n",
    "               \n",
    "        for hy in hypo_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hypo_list) != 0):\n",
    "            print (\"Bleu score for hyponyms (1-gram):\", bleu_count / len(hypo_list))\n",
    "            print (\"Rogue f1 for hyponyms (1-gram):\", f_count / len(hypo_list))\n",
    "        else:\n",
    "            print(\"No hyponyms\")\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d6b0e43-226e-49c3-b8a6-fae5e1ee8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Definition of Synset('courage.n.01') = a quality of spirit that enables you to face danger or pain without showing fear\n",
      "\n",
      "Bleu score for hypernyms (1-gram): 0.6990638273805326\n",
      "Rogue f1 for hypernyms (1-gram): 0.7164179054488752\n",
      "\n",
      "Bleu score for hyponyms (1-gram): 0.32899310492431155\n",
      "Rogue f1 for hyponyms (1-gram): 0.5992050036619138\n"
     ]
    }
   ],
   "source": [
    "definition_overlap(\"courage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb7ef8b-82e9-4774-b7b4-83ffb7bf5067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Synset('paper.n.01'): {'paper': 6,\n",
       "  'material': 1,\n",
       "  'cellulose': 9,\n",
       "  'pulp': 6,\n",
       "  'derived': 1,\n",
       "  'wood': 11,\n",
       "  'rags': 5,\n",
       "  'grasses': 5},\n",
       " Synset('composition.n.08'): {'paper': 7,\n",
       "  'essay': 3,\n",
       "  'written': 1,\n",
       "  'assignment': 9},\n",
       " Synset('newspaper.n.01'): {'paper': 10,\n",
       "  'daily': 1,\n",
       "  'weekly': 1,\n",
       "  'publication': 9,\n",
       "  'folded': 2,\n",
       "  'sheets': 2,\n",
       "  'news': 7,\n",
       "  'articles': 5,\n",
       "  'advertisements': 6},\n",
       " Synset('paper.n.04'): {'paper': 8,\n",
       "  'medium': 1,\n",
       "  'written': 1,\n",
       "  'communication': 5},\n",
       " Synset('paper.n.05'): {'paper': 9,\n",
       "  'scholarly': 1,\n",
       "  'article': 5,\n",
       "  'describing': 1,\n",
       "  'observations': 8,\n",
       "  'stating': 3,\n",
       "  'hypotheses': 6},\n",
       " Synset('newspaper.n.02'): {'paper': 10,\n",
       "  'business': 8,\n",
       "  'firm': 1,\n",
       "  'publishes': 3,\n",
       "  'newspapers': 7},\n",
       " Synset('newspaper.n.03'): {'paper': 8,\n",
       "  'physical': 1,\n",
       "  'object': 2,\n",
       "  'product': 6,\n",
       "  'newspaper': 7,\n",
       "  'publisher': 9},\n",
       " Synset('paper.v.01'): {'paper': 2, 'cover': 4},\n",
       " Synset('wallpaper.v.01'): {'paper': 2, 'cover': 4, 'wallpaper': 2}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_root(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acc2bd-a698-43e0-a59b-e7313aa7026f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
