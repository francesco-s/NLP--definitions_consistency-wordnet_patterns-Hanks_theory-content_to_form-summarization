{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc6c91c-de97-4491-98a4-25202da67cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef5d635-95ad-4fd8-84e7-29a40503a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(remove_punctuation(word_tokenize(sentence)))\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(words_list):\n",
    "    new_words_list = []\n",
    "    for word in words_list:\n",
    "        temp = word\n",
    "        if not temp.strip(string.punctuation) == \"\":\n",
    "            new_word = word.lower()\n",
    "            new_word = new_word.replace(\"'\", \"\")\n",
    "            new_words_list.append(new_word)\n",
    "    return new_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88648d7-c858-4786-b49f-24068483d4ab",
   "metadata": {},
   "source": [
    "#### First exercize\n",
    "It calculates average definition lenght for each section (nouns, verbs, adjectives and adverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae19b6e-e10a-4b85-97c3-8cec8e5378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def avg_len_section_definitons():\n",
    "    pos_tag_list = ['n', 'v', 'a', 'r']\n",
    "    average_lenghts = []\n",
    "\n",
    "    for pos_tag in pos_tag_list:\n",
    "        synsets_lenght = []\n",
    "        for synset in list(wn.all_synsets(pos_tag)):\n",
    "            synsets_lenght.append(len(synset.definition().split(\" \")))\n",
    "        average_lenghts.append((pos_tag, mean(synsets_lenght)))\n",
    "\n",
    "    print(\"\\n\", average_lenghts, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a398d-e50c-41be-8fd5-1129d23a25ba",
   "metadata": {},
   "source": [
    "#### Second exercize\n",
    "The variation of the length along the path of the hyperonyms that lead from a given synset to its root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdd85c8-d997-466a-94c5-d1cb641669fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypernym_paths(word):\n",
    "    \n",
    "    def_lens = []\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "\n",
    "        single_path = []\n",
    "        \n",
    "        hyp_path = syn.hypernym_paths()\n",
    "        \n",
    "        for i in range (0, len(hyp_path[0])):\n",
    "            \n",
    "            single_path.append((hyp_path[0][i],len((hyp_path[0][i].definition()).split())))\n",
    "\n",
    "        def_lens.append(single_path)\n",
    "        \n",
    "    return def_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb97c5f-143d-4c46-af14-cdf072f35c26",
   "metadata": {},
   "source": [
    "#### Third exercize\n",
    "Distance from the word's root and words within the definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c60b5d8-8368-4c47-99dd-a33b349eff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_root(synset):\n",
    "    return (min([len(path) for path in synset.hypernym_paths()]))\n",
    "\n",
    "def distance_root(word):\n",
    "    \n",
    "    output = dict()\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        actual_syn_dis = calculate_distance_root(syn)\n",
    "        output[syn] = {word :actual_syn_dis} \n",
    "                \n",
    "        syn_definition_processed = pre_processing(syn.definition())\n",
    "        for def_word in syn_definition_processed:\n",
    "            for def_syn in wn.synsets(def_word):\n",
    "                output[syn].update({def_word : calculate_distance_root(def_syn)})\n",
    "                        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a7afe-0b0b-4ab8-903e-63839a16a3f0",
   "metadata": {},
   "source": [
    "#### Fourth exercize\n",
    "Calculate bleu and rouges score between hypernyms and hyponyms of the words definition  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bcb3da-320a-46bf-904d-1616f7b34952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from bleu import multi_list_bleu\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def definition_overlap(word):\n",
    "    \n",
    "    for syn in wn.synsets(word):\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "        \n",
    "        actual_def_processed = syn.definition()\n",
    "        \n",
    "        print (\"\\n--------------------\\nDefinition of\", syn,  \"=\", actual_def_processed)\n",
    "        print ()\n",
    "        \n",
    "        hyper_list = syn.hypernyms()\n",
    "        \n",
    "        for hy in hyper_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hyper_list) != 0):\n",
    "            print (\"Bleu score for hypernyms (1-gram):\", bleu_count / len(hyper_list))\n",
    "            print (\"Rogue f1 for hypernyms (1-gram):\", f_count / len(hyper_list))\n",
    "        else:\n",
    "            print(\"No hypernyms\")\n",
    "\n",
    "               \n",
    "        print ()\n",
    "        \n",
    "        bleu_count = 0\n",
    "        f_count = 0\n",
    "\n",
    "        hypo_list = syn.hyponyms()\n",
    "               \n",
    "        for hy in hypo_list:\n",
    "            hy_def = hy.definition()\n",
    "            \n",
    "            bleu_count += sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0))\n",
    "            #print(\"BLEU score: \", sentence_bleu([actual_def_processed], hy_def, weights=(1, 0, 0, 0)))\n",
    "            \n",
    "            rouge_scores = rouge.get_scores(' '.join(hy_def), ' '.join(actual_def_processed))\n",
    "            #print(\"Rogue scores: \", rouge_scores)\n",
    "            f_count += rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "        if (len(hypo_list) != 0):\n",
    "            print (\"Bleu score for hyponyms (1-gram):\", bleu_count / len(hypo_list))\n",
    "            print (\"Rogue f1 for hyponyms (1-gram):\", f_count / len(hypo_list))\n",
    "        else:\n",
    "            print(\"No hyponyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156e046d-1568-48a8-b0a1-24c50baeff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [('n', 11.470035925226815), ('v', 6.146655044672042), ('a', 7.238433575677462), ('r', 5.028169014084507)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_len_section_definitons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb7ef8b-82e9-4774-b7b4-83ffb7bf5067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('matter.n.03'), 7),\n",
       "  (Synset('substance.n.01'), 11),\n",
       "  (Synset('material.n.01'), 12),\n",
       "  (Synset('paper.n.01'), 15)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('communication.n.02'), 12),\n",
       "  (Synset('written_communication.n.01'), 10),\n",
       "  (Synset('writing.n.02'), 24),\n",
       "  (Synset('essay.n.01'), 6),\n",
       "  (Synset('composition.n.08'), 8)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('instrumentality.n.03'), 13),\n",
       "  (Synset('medium.n.01'), 9),\n",
       "  (Synset('print_media.n.01'), 6),\n",
       "  (Synset('press.n.02'), 16),\n",
       "  (Synset('newspaper.n.01'), 14)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('instrumentality.n.03'), 13),\n",
       "  (Synset('medium.n.01'), 9),\n",
       "  (Synset('paper.n.04'), 5)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('communication.n.02'), 12),\n",
       "  (Synset('expressive_style.n.01'), 25),\n",
       "  (Synset('writing_style.n.01'), 7),\n",
       "  (Synset('prose.n.01'), 6),\n",
       "  (Synset('nonfiction.n.01'), 6),\n",
       "  (Synset('article.n.01'), 9),\n",
       "  (Synset('paper.n.05'), 11)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('abstraction.n.06'), 11),\n",
       "  (Synset('group.n.01'), 9),\n",
       "  (Synset('social_group.n.01'), 5),\n",
       "  (Synset('organization.n.01'), 7),\n",
       "  (Synset('enterprise.n.02'), 6),\n",
       "  (Synset('business.n.01'), 11),\n",
       "  (Synset('firm.n.01'), 14),\n",
       "  (Synset('publisher.n.01'), 6),\n",
       "  (Synset('newspaper.n.02'), 6)],\n",
       " [(Synset('entity.n.01'), 17),\n",
       "  (Synset('physical_entity.n.01'), 6),\n",
       "  (Synset('object.n.01'), 12),\n",
       "  (Synset('whole.n.02'), 11),\n",
       "  (Synset('artifact.n.01'), 7),\n",
       "  (Synset('creation.n.02'), 10),\n",
       "  (Synset('product.n.02'), 11),\n",
       "  (Synset('newspaper.n.03'), 11)],\n",
       " [(Synset('cover.v.01'), 9), (Synset('paper.v.01'), 3)],\n",
       " [(Synset('cover.v.01'), 9), (Synset('wallpaper.v.01'), 3)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypernym_paths(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "332db380-cb06-4da6-bdfd-29fd9080073c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Synset('courage.n.01'): {'courage': 7,\n",
       "  'quality': 1,\n",
       "  'spirit': 5,\n",
       "  'enables': 2,\n",
       "  'face': 2,\n",
       "  'danger': 7,\n",
       "  'pain': 4,\n",
       "  'showing': 3,\n",
       "  'fear': 6}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_root(\"courage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6b0e43-226e-49c3-b8a6-fae5e1ee8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "Definition of Synset('courage.n.01') = a quality of spirit that enables you to face danger or pain without showing fear\n",
      "\n",
      "Bleu score for hypernyms (1-gram): 0.6990638273805326\n",
      "Rogue f1 for hypernyms (1-gram): 0.8205128155161079\n",
      "\n",
      "Bleu score for hyponyms (1-gram): 0.32899310492431155\n",
      "Rogue f1 for hyponyms (1-gram): 0.7829971823603259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thewp\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\thewp\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "definition_overlap(\"courage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acc2bd-a698-43e0-a59b-e7313aa7026f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
