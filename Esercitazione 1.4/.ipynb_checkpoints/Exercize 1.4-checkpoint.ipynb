{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe4662-5df7-420d-83c7-ae325e1c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea48f5b-172a-4146-b548-4fb94a9abfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    \n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    \n",
    "    new_words_list = []\n",
    "    for word in words_list:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stopwords_list:\n",
    "            new_words_list.append(word_lower)\n",
    "    return new_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15731043-2c31-4392-b39d-da6b1f7195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # tokenizzo la definizione del synset\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # tokenizzo ogni esempio del synset\n",
    "        for word in tokenize_sentence(example):\n",
    "            signature.append(word)\n",
    "    return signature  # la signature conterrà tutte le parole presenti nella definizione del senso e negli esempi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f409890b-bd8b-4452-9352-54ee468fa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizza la frase in input e ne affettua anche la lemmatizzazione della sue parole\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(str(sentence))):\n",
    "        if tag[1][:2] == \"NN\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif tag[1][:2] == \"VB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif tag[1][:2] == \"RB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif tag[1][:2] == \"JJ\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92389c8c-69f2-4872-a8c7-aacfae07151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e381080f-09d5-4ec3-bcd1-e1ae3b3442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un soggetto cerca l'oggetto della frase rispetto al verbo considerato\n",
    "def search_obj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'dobj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency2 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f3f446-f0f2-41ee-914d-1ea45c97834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un oggetto cerca il soggetto della frase rispetto al verbo considerato\n",
    "def search_subj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50791314-a0bb-4bb2-aa2d-3f0634e29fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON UTILIZZATO (però si può usare). Algortimo di Lesk fatto da me\n",
    "# ottiene risultati diversi perchè è diversa l'implementazione rispetto a quello\n",
    "# di nltk\n",
    "\n",
    "# Data una parola ambigua e il contesto in cui la parola si verifica,\n",
    "# Lesk restituisce un Synset con il maggior numero di parole sovrapposte tra la frase\n",
    "# di contesto e definizioni diverse da ciascun Synset.\n",
    "def lesk_algorithm(word, sentence_words):\n",
    "    if len(wn.synsets(word)) != 0:  # se non esiste in wordnet\n",
    "        best_sense = wn.synsets(word)[0]\n",
    "        max_overlap = 0\n",
    "        context = remove_stopwords(sentence_words)\n",
    "        for sense in wn.synsets(word):\n",
    "            signature = remove_stopwords(get_signature(sense))\n",
    "            overlap = len(list(set(signature) & set(context)))  # overlap\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_sense = sense\n",
    "        return best_sense\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7656542-725c-4231-a828-84e8f5560a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prende in input un pattern e restituisce un nuovo pattern\n",
    "# composto come segue: word_super_sense1 , word_super_sense2\n",
    "# in pratica disambigua i termini in base alla sentence a cui fanno riferimento\n",
    "def disambiguate_terms(pattern):\n",
    "    dependency1 = pattern[0]\n",
    "    dependency2 = pattern[1]\n",
    "\n",
    "    sentence = pattern[2]\n",
    "\n",
    "    word1 = dependency1[0]\n",
    "    word2 = dependency2[0]\n",
    "\n",
    "    \"\"\"\n",
    "    sense1 = lesk_algorithm(word1, tokenize_sentence(sentence))\n",
    "    sense2 = lesk_algorithm(word2, tokenize_sentence(sentence))\n",
    "    \"\"\"\n",
    "    # disambiguazione dei significati\n",
    "    sense1 = lesk(tokenize_sentence(sentence), word1, 'n')\n",
    "    sense2 = lesk(tokenize_sentence(sentence), word2, 'n')\n",
    "\n",
    "    # nel new poattern ci saranno i supersensi di sense1 e sense2\n",
    "    if sense1 and sense2:\n",
    "        new_pattern = sense1._lexname, sense2._lexname\n",
    "        return new_pattern\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc99bd3-3621-445d-8e2b-5a5651f8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restituisce i cluster semantici\n",
    "def compute_semantic_clusters(patterns):\n",
    "    new_patterns = []\n",
    "    for pattern in patterns:\n",
    "        new_pattern = disambiguate_terms(pattern)\n",
    "        if new_pattern:\n",
    "            # supersense1, supersense2\n",
    "            new_patterns.append(new_pattern)\n",
    "\n",
    "    # calcolo percentuali rispetto al numero di nuovi pattern trovati\n",
    "    results = dict(Counter(new_patterns))\n",
    "    semantic_clusters = []\n",
    "    for key in results.keys():\n",
    "        result = results[key]\n",
    "        percentage = result / len(new_patterns)\n",
    "        cluster = key, format(percentage * 100, '.2f') + '%'\n",
    "        semantic_clusters.append(cluster)\n",
    "\n",
    "    # ordino in maniera decrescente in base alla frequenza(percentuale)\n",
    "    semantic_clusters = sorted(semantic_clusters, key=lambda x: x[1], reverse=True)\n",
    "    return semantic_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3ba60c2-9b84-4ce7-8ce2-974fff170783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822005c1-0634-4295-93de-90509329af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VERB = \"buy\"\n",
    "CORPUS = \"buy_corpus.xml\"\n",
    "all_verb_forms = ['buy', 'buys']\n",
    "\n",
    "\"\"\"\n",
    "VERB = \"eat\"\n",
    "CORPUS = \"eat_corpus.xml\"\n",
    "all_verb_forms = ['eat', 'eats']\n",
    "\"\"\"\n",
    "ARGUMENTS_N = 2  # numero di argomenti del verbo (VALENZA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2078c7cf-bdff-4252-9cf2-040498920bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbo:  buy\n",
      "Forme del verbo:  ['buy', 'buys']\n",
      "Numero di pattern trovati:  2683\n",
      "(('noun.quantity', 'noun.cognition'), '7.48%')\n",
      "(('noun.group', 'noun.artifact'), '4.61%')\n",
      "(('noun.quantity', 'noun.communication'), '3.89%')\n",
      "(('noun.person', 'noun.artifact'), '3.59%')\n",
      "(('noun.act', 'noun.substance'), '2.66%')\n",
      "(('noun.group', 'noun.communication'), '2.25%')\n",
      "(('noun.substance', 'noun.artifact'), '2.25%')\n",
      "(('noun.quantity', 'noun.artifact'), '11.68%')\n",
      "(('noun.quantity', 'noun.person'), '1.95%')\n",
      "(('noun.quantity', 'noun.act'), '1.84%')\n",
      "(('noun.person', 'noun.cognition'), '1.84%')\n",
      "(('noun.quantity', 'noun.food'), '1.74%')\n",
      "(('noun.quantity', 'noun.attribute'), '1.64%')\n",
      "(('noun.artifact', 'noun.artifact'), '1.43%')\n",
      "(('noun.person', 'noun.communication'), '1.43%')\n",
      "(('noun.quantity', 'noun.quantity'), '1.33%')\n",
      "(('noun.communication', 'noun.artifact'), '1.33%')\n",
      "(('noun.person', 'noun.attribute'), '1.23%')\n",
      "(('noun.cognition', 'noun.artifact'), '1.23%')\n",
      "(('noun.location', 'noun.artifact'), '1.13%')\n",
      "(('noun.group', 'noun.possession'), '1.13%')\n",
      "(('noun.group', 'noun.cognition'), '1.13%')\n",
      "(('noun.quantity', 'noun.group'), '1.13%')\n",
      "(('noun.person', 'noun.phenomenon'), '1.02%')\n",
      "(('noun.group', 'noun.person'), '1.02%')\n",
      "(('noun.quantity', 'noun.phenomenon'), '1.02%')\n",
      "(('noun.act', 'noun.artifact'), '0.92%')\n",
      "(('noun.group', 'noun.act'), '0.92%')\n",
      "(('noun.group', 'noun.phenomenon'), '0.82%')\n",
      "(('noun.group', 'noun.food'), '0.82%')\n",
      "(('noun.group', 'noun.attribute'), '0.82%')\n",
      "(('noun.quantity', 'noun.possession'), '0.82%')\n",
      "(('noun.substance', 'noun.cognition'), '0.72%')\n",
      "(('noun.artifact', 'noun.cognition'), '0.72%')\n",
      "(('noun.substance', 'noun.communication'), '0.72%')\n",
      "(('noun.substance', 'noun.substance'), '0.72%')\n",
      "(('noun.quantity', 'noun.substance'), '0.72%')\n",
      "(('noun.artifact', 'noun.communication'), '0.72%')\n",
      "(('noun.group', 'noun.group'), '0.72%')\n",
      "(('noun.cognition', 'noun.cognition'), '0.61%')\n",
      "(('noun.possession', 'noun.feeling'), '0.61%')\n",
      "(('noun.quantity', 'noun.plant'), '0.61%')\n",
      "(('noun.group', 'noun.plant'), '0.61%')\n",
      "(('noun.possession', 'noun.communication'), '0.51%')\n",
      "(('noun.person', 'noun.possession'), '0.51%')\n",
      "(('noun.person', 'noun.animal'), '0.51%')\n",
      "(('noun.quantity', 'noun.location'), '0.51%')\n",
      "(('noun.person', 'noun.time'), '0.51%')\n",
      "(('noun.Tops', 'noun.artifact'), '0.51%')\n",
      "(('noun.communication', 'noun.cognition'), '0.51%')\n",
      "(('noun.person', 'noun.group'), '0.51%')\n",
      "(('noun.attribute', 'noun.artifact'), '0.51%')\n",
      "(('noun.quantity', 'noun.Tops'), '0.51%')\n",
      "(('noun.group', 'noun.Tops'), '0.51%')\n",
      "(('noun.communication', 'noun.attribute'), '0.51%')\n",
      "(('noun.group', 'noun.location'), '0.51%')\n",
      "(('noun.substance', 'noun.possession'), '0.51%')\n",
      "(('noun.person', 'noun.food'), '0.41%')\n",
      "(('noun.quantity', 'noun.animal'), '0.41%')\n",
      "(('noun.communication', 'noun.communication'), '0.41%')\n",
      "(('noun.substance', 'noun.person'), '0.41%')\n",
      "(('noun.quantity', 'noun.process'), '0.41%')\n",
      "(('noun.quantity', 'noun.state'), '0.31%')\n",
      "(('noun.artifact', 'noun.possession'), '0.31%')\n",
      "(('noun.cognition', 'noun.person'), '0.31%')\n",
      "(('noun.person', 'noun.quantity'), '0.31%')\n",
      "(('noun.quantity', 'noun.object'), '0.31%')\n",
      "(('noun.location', 'noun.communication'), '0.31%')\n",
      "(('noun.cognition', 'noun.communication'), '0.31%')\n",
      "(('noun.person', 'noun.plant'), '0.31%')\n",
      "(('noun.artifact', 'noun.substance'), '0.31%')\n",
      "(('noun.person', 'noun.person'), '0.31%')\n",
      "(('noun.substance', 'noun.act'), '0.31%')\n",
      "(('noun.cognition', 'noun.act'), '0.31%')\n",
      "(('noun.cognition', 'noun.substance'), '0.31%')\n",
      "(('noun.group', 'noun.body'), '0.31%')\n",
      "(('noun.communication', 'noun.act'), '0.31%')\n",
      "(('noun.quantity', 'noun.body'), '0.20%')\n",
      "(('noun.cognition', 'noun.group'), '0.20%')\n",
      "(('noun.Tops', 'noun.communication'), '0.20%')\n",
      "(('noun.possession', 'noun.act'), '0.20%')\n",
      "(('noun.substance', 'noun.food'), '0.20%')\n",
      "(('noun.person', 'noun.act'), '0.20%')\n",
      "(('noun.attribute', 'noun.quantity'), '0.20%')\n",
      "(('noun.object', 'noun.artifact'), '0.20%')\n",
      "(('noun.substance', 'noun.attribute'), '0.20%')\n",
      "(('noun.location', 'noun.Tops'), '0.20%')\n",
      "(('noun.state', 'noun.artifact'), '0.20%')\n",
      "(('noun.body', 'noun.person'), '0.20%')\n",
      "(('noun.quantity', 'noun.event'), '0.20%')\n",
      "(('noun.communication', 'noun.group'), '0.20%')\n",
      "(('noun.cognition', 'noun.possession'), '0.20%')\n",
      "(('noun.act', 'noun.act'), '0.20%')\n",
      "(('noun.person', 'noun.state'), '0.20%')\n",
      "(('noun.act', 'noun.communication'), '0.20%')\n",
      "(('noun.group', 'noun.substance'), '0.20%')\n",
      "(('noun.substance', 'noun.animal'), '0.20%')\n",
      "(('noun.location', 'noun.quantity'), '0.20%')\n",
      "(('noun.location', 'noun.phenomenon'), '0.20%')\n",
      "(('noun.location', 'noun.act'), '0.20%')\n",
      "(('noun.quantity', 'noun.time'), '0.20%')\n",
      "(('noun.possession', 'noun.quantity'), '0.20%')\n",
      "(('noun.possession', 'noun.artifact'), '0.20%')\n",
      "(('noun.animal', 'noun.artifact'), '0.20%')\n",
      "(('noun.substance', 'noun.relation'), '0.20%')\n",
      "(('noun.artifact', 'noun.act'), '0.10%')\n",
      "(('noun.act', 'noun.attribute'), '0.10%')\n",
      "(('noun.animal', 'noun.group'), '0.10%')\n",
      "(('noun.state', 'noun.cognition'), '0.10%')\n",
      "(('noun.cognition', 'noun.quantity'), '0.10%')\n",
      "(('noun.relation', 'noun.body'), '0.10%')\n",
      "(('noun.substance', 'noun.group'), '0.10%')\n",
      "(('noun.process', 'noun.artifact'), '0.10%')\n",
      "(('noun.artifact', 'noun.quantity'), '0.10%')\n",
      "(('noun.possession', 'noun.possession'), '0.10%')\n",
      "(('noun.act', 'noun.location'), '0.10%')\n",
      "(('noun.food', 'noun.communication'), '0.10%')\n",
      "(('noun.person', 'noun.Tops'), '0.10%')\n",
      "(('noun.time', 'noun.quantity'), '0.10%')\n",
      "(('noun.substance', 'noun.phenomenon'), '0.10%')\n",
      "(('noun.attribute', 'noun.location'), '0.10%')\n",
      "(('noun.substance', 'noun.object'), '0.10%')\n",
      "(('noun.state', 'noun.quantity'), '0.10%')\n",
      "(('noun.cognition', 'noun.state'), '0.10%')\n",
      "(('noun.group', 'noun.quantity'), '0.10%')\n",
      "(('noun.Tops', 'noun.time'), '0.10%')\n",
      "(('noun.group', 'noun.event'), '0.10%')\n",
      "(('noun.group', 'noun.state'), '0.10%')\n",
      "(('noun.substance', 'noun.quantity'), '0.10%')\n",
      "(('noun.location', 'noun.cognition'), '0.10%')\n",
      "(('noun.location', 'noun.group'), '0.10%')\n",
      "(('noun.artifact', 'noun.event'), '0.10%')\n",
      "(('noun.attribute', 'noun.substance'), '0.10%')\n",
      "(('noun.animal', 'noun.cognition'), '0.10%')\n",
      "(('noun.person', 'noun.substance'), '0.10%')\n",
      "(('noun.phenomenon', 'noun.phenomenon'), '0.10%')\n",
      "(('noun.substance', 'noun.location'), '0.10%')\n",
      "(('noun.attribute', 'noun.act'), '0.10%')\n",
      "(('noun.possession', 'noun.attribute'), '0.10%')\n",
      "(('noun.location', 'noun.location'), '0.10%')\n",
      "(('noun.group', 'noun.time'), '0.10%')\n",
      "(('noun.animal', 'noun.time'), '0.10%')\n",
      "(('noun.artifact', 'noun.plant'), '0.10%')\n",
      "(('noun.location', 'noun.person'), '0.10%')\n",
      "(('noun.body', 'noun.artifact'), '0.10%')\n",
      "(('noun.body', 'noun.communication'), '0.10%')\n",
      "(('noun.possession', 'noun.person'), '0.10%')\n",
      "(('noun.plant', 'noun.food'), '0.10%')\n",
      "(('noun.artifact', 'noun.group'), '0.10%')\n",
      "(('noun.Tops', 'noun.attribute'), '0.10%')\n",
      "(('noun.state', 'noun.act'), '0.10%')\n",
      "(('noun.location', 'noun.animal'), '0.10%')\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    mydoc = minidom.parse(CORPUS)\n",
    "\n",
    "    # trova tutte le frasi del corpus\n",
    "    sentences = mydoc.getElementsByTagName('line')\n",
    "\n",
    "    print(\"Verbo: \", VERB)\n",
    "    print(\"Forme del verbo: \", all_verb_forms)\n",
    "\n",
    "    patterns = []\n",
    "    for item in sentences:\n",
    "        # nlp effettua un parsing che restituisce una struttura ad albero a dipendenze\n",
    "        sentence = nlp(item.firstChild.data.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"     \", \"\"))\n",
    "        pattern = []\n",
    "        for token in sentence:\n",
    "            # se trova il soggetto cerca l'oggetto\n",
    "            if token.head.text in all_verb_forms and token.dep_ == 'nsubj' and len(\n",
    "                    pattern) < ARGUMENTS_N and token.text != '\\n  ' and token.text != '\\n':\n",
    "                dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "                pattern.append(dependency1)\n",
    "                dependency2 = search_obj(sentence, token.head.text, pattern)\n",
    "                if dependency2:\n",
    "                    pattern.append(dependency2)\n",
    "        if len(pattern) == ARGUMENTS_N:  # se ho trovato tutti e due gli argomenti (soggetto, oggetto) allora va bene\n",
    "            pattern.append(sentence)\n",
    "            patterns.append(pattern)\n",
    "\n",
    "    print(\"Numero di pattern trovati: \", len(patterns))\n",
    "    # calcolo i cluster semantici con le loro percentuali di frequenza\n",
    "    semantic_clusters = compute_semantic_clusters(patterns)\n",
    "    for cluster in semantic_clusters:\n",
    "        print(cluster)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
