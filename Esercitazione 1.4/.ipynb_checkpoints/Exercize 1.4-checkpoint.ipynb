{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe4662-5df7-420d-83c7-ae325e1c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea48f5b-172a-4146-b548-4fb94a9abfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e381080f-09d5-4ec3-bcd1-e1ae3b3442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un soggetto cerca l'oggetto della frase rispetto al verbo considerato\n",
    "def search_obj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'dobj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency2 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f3f446-f0f2-41ee-914d-1ea45c97834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un oggetto cerca il soggetto della frase rispetto al verbo considerato\n",
    "def search_subj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7656542-725c-4231-a828-84e8f5560a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prende in input un pattern e restituisce un nuovo pattern\n",
    "# composto come segue: word_super_sense1 , word_super_sense2\n",
    "# in pratica disambigua i termini in base alla sentence a cui fanno riferimento\n",
    "def disambiguate_terms(pattern):\n",
    "    dep1 = pattern[0]\n",
    "    dep2 = pattern[1]\n",
    "\n",
    "    sentence = pattern[2]\n",
    "\n",
    "    w1 = dep1[0]\n",
    "    w2 = dep2[0]\n",
    "\n",
    "    # WSD with nltk Lesk\n",
    "    return lesk(tokenize_sentence(sentence), w1, 'n'), lesk(tokenize_sentence(sentence), w2, 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc99bd3-3621-445d-8e2b-5a5651f8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates semantic clusters\n",
    "def compute_semantic_clusters(patterns):\n",
    "    new_patterns = []\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        best_sense1, best_sense2 = disambiguate_terms(pattern)\n",
    "        \n",
    "        if best_sense1 and best_sense2:\n",
    "            # supersense1, supersense2\n",
    "            new_patterns.append((best_sense1._lexname, best_sense2._lexname))\n",
    "\n",
    "    # Create a dict of Counter. Res like ((supersense1, supersensse2), frequency)\n",
    "    patterns_Counter = dict(Counter(new_patterns))\n",
    "    semantic_clusters = []\n",
    "    \n",
    "    for key in patterns_Counter.keys():\n",
    "        result = patterns_Counter[key]\n",
    "        percentage = result / len(new_patterns)\n",
    "        # Cluster is a grouping of (subject, object) tuples with frequency associated\n",
    "        cluster = key, format(percentage * 100, '.2f') + '%'\n",
    "        semantic_clusters.append(cluster)\n",
    "\n",
    "    # sort clusters by frequency\n",
    "    semantic_clusters = sorted(semantic_clusters, key=lambda x: x[1], reverse=True)\n",
    "    return semantic_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078c7cf-bdff-4252-9cf2-040498920bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb:  buy\n",
      "Verb forms:  ['buy', 'buys']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from xml.dom import minidom\n",
    "\n",
    "VERB = \"buy\"\n",
    "CORPUS = \"buy_corpus.xml\"\n",
    "all_verb_forms = ['buy', 'buys']\n",
    "\n",
    "\"\"\"\n",
    "VERB = \"eat\"\n",
    "CORPUS = \"eat_corpus.xml\"\n",
    "all_verb_forms = ['eat', 'eats']\n",
    "\"\"\"\n",
    "\n",
    "# Valenza\n",
    "VERB_ARGUMENTS = 2\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use minidom to parse XML file\n",
    "mydoc = minidom.parse(CORPUS)\n",
    "\n",
    "# Obtain all sentences \n",
    "sentences = mydoc.getElementsByTagName('line')\n",
    "\n",
    "print(\"Verb: \", VERB)\n",
    "print(\"Verb forms: \", all_verb_forms)\n",
    "\n",
    "patterns = []\n",
    "for sentence in sentences:\n",
    "    # nlp do dependecies parsing and PoS\n",
    "    sentence_parsed = nlp(sentence.firstChild.data.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"     \", \"\"))\n",
    "    pattern = []\n",
    "\n",
    "    for token in sentence_parsed:\n",
    "        # if token is a subject we can find subjects\n",
    "        if token.head.text in all_verb_forms and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < VERB_ARGUMENTS and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            pattern.append(dependency1)\n",
    "            dependency2 = search_obj(sentence_parsed, token.head.text, pattern)\n",
    "            if dependency2:\n",
    "                pattern.append(dependency2)\n",
    "    if len(pattern) == VERB_ARGUMENTS:  # append if we have 2 arguments\n",
    "        pattern.append(sentence_parsed)\n",
    "        patterns.append(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7577133-86e6-4682-8088-788ffe353da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of patterns found: \", len(patterns))\n",
    "# Obtain semantic clusters with frequency \n",
    "semantic_clusters = compute_semantic_clusters(patterns)\n",
    "\n",
    "# Check if probabilities are consistent\n",
    "print(sum(float(semantic_clusters[i][1][0:-1]) for i in range(0, len(semantic_clusters))))\n",
    "\n",
    "# Print clusters\n",
    "for cluster in semantic_clusters:\n",
    "    print(cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
