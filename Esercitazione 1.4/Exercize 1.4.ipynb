{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe4662-5df7-420d-83c7-ae325e1c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea48f5b-172a-4146-b548-4fb94a9abfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    \n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    \n",
    "    new_words_list = []\n",
    "    for word in words_list:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower not in stopwords_list:\n",
    "            new_words_list.append(word_lower)\n",
    "    return new_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15731043-2c31-4392-b39d-da6b1f7195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f409890b-bd8b-4452-9352-54ee468fa513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizza la frase in input e ne affettua anche la lemmatizzazione della sue parole\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(str(sentence))):\n",
    "        if tag[1][:2] == \"NN\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif tag[1][:2] == \"VB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif tag[1][:2] == \"RB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif tag[1][:2] == \"JJ\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92389c8c-69f2-4872-a8c7-aacfae07151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e381080f-09d5-4ec3-bcd1-e1ae3b3442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un soggetto cerca l'oggetto della frase rispetto al verbo considerato\n",
    "def search_obj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'dobj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency2 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f3f446-f0f2-41ee-914d-1ea45c97834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dato un oggetto cerca il soggetto della frase rispetto al verbo considerato\n",
    "def search_subj(sentence, head_verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == head_verb and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dependency1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7656542-725c-4231-a828-84e8f5560a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prende in input un pattern e restituisce un nuovo pattern\n",
    "# composto come segue: word_super_sense1 , word_super_sense2\n",
    "# in pratica disambigua i termini in base alla sentence a cui fanno riferimento\n",
    "def disambiguate_terms(pattern):\n",
    "    dep1 = pattern[0]\n",
    "    dep2 = pattern[1]\n",
    "\n",
    "    sentence = pattern[2]\n",
    "\n",
    "    w1 = dep1[0]\n",
    "    w2 = dep2[0]\n",
    "\n",
    "    # WSD with nltk Lesk\n",
    "    return lesk(tokenize_sentence(sentence), w1, 'n'), lesk(tokenize_sentence(sentence), w2, 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc99bd3-3621-445d-8e2b-5a5651f8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates semantic clusters\n",
    "def compute_semantic_clusters(patterns):\n",
    "    new_patterns = []\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        best_sense1, best_sense2 = disambiguate_terms(pattern)\n",
    "        \n",
    "        if best_sense1 and best_sense2:\n",
    "            # supersense1, supersense2\n",
    "            new_patterns.append((best_sense1._lexname, best_sense2._lexname))\n",
    "\n",
    "    # Create a dict of Counter. Res like ((supersense1, supersensse2), frequency)\n",
    "    patterns_Counter = dict(Counter(new_patterns))\n",
    "    semantic_clusters = []\n",
    "    \n",
    "    for key in patterns_Counter.keys():\n",
    "        result = patterns_Counter[key]\n",
    "        percentage = result / len(new_patterns)\n",
    "        # Cluster is a grouping of (subject, object) tuples with frequency associated\n",
    "        cluster = key, format(percentage * 100, '.2f') + '%'\n",
    "        semantic_clusters.append(cluster)\n",
    "\n",
    "    # sort clusters by frequency\n",
    "    semantic_clusters = sorted(semantic_clusters, key=lambda x: x[1], reverse=True)\n",
    "    return semantic_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ba60c2-9b84-4ce7-8ce2-974fff170783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822005c1-0634-4295-93de-90509329af75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VERB = \"buy\"\n",
    "CORPUS = \"buy_corpus.xml\"\n",
    "all_verb_forms = ['buy', 'buys']\n",
    "\n",
    "\"\"\"\n",
    "VERB = \"eat\"\n",
    "CORPUS = \"eat_corpus.xml\"\n",
    "all_verb_forms = ['eat', 'eats']\n",
    "\"\"\"\n",
    "# Valenza\n",
    "VERB_ARGUMENTS = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2078c7cf-bdff-4252-9cf2-040498920bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb:  buy\n",
      "Verb forms:  ['buy', 'buys']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Use minidom to parse XML file\n",
    "mydoc = minidom.parse(CORPUS)\n",
    "\n",
    "# Obtain all sentences \n",
    "sentences = mydoc.getElementsByTagName('line')\n",
    "\n",
    "print(\"Verb: \", VERB)\n",
    "print(\"Verb forms: \", all_verb_forms)\n",
    "\n",
    "patterns = []\n",
    "for sentence in sentences:\n",
    "    # nlp do dependecies parsing and PoS\n",
    "    sentence_parsed = nlp(sentence.firstChild.data.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"     \", \"\"))\n",
    "    pattern = []\n",
    "\n",
    "    for token in sentence_parsed:\n",
    "        # if token is a subject we can find subjects\n",
    "        if token.head.text in all_verb_forms and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < VERB_ARGUMENTS and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dependency1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            pattern.append(dependency1)\n",
    "            dependency2 = search_obj(sentence_parsed, token.head.text, pattern)\n",
    "            if dependency2:\n",
    "                pattern.append(dependency2)\n",
    "    if len(pattern) == VERB_ARGUMENTS:  # append iff we have 2 arguments\n",
    "        pattern.append(sentence_parsed)\n",
    "        patterns.append(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7577133-86e6-4682-8088-788ffe353da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di pattern trovati:  2709\n",
      "99.8599999999997\n",
      "(('noun.quantity', 'noun.cognition'), '7.12%')\n",
      "(('noun.group', 'noun.artifact'), '4.71%')\n",
      "(('noun.quantity', 'noun.communication'), '3.71%')\n",
      "(('noun.person', 'noun.artifact'), '3.71%')\n",
      "(('noun.act', 'noun.substance'), '2.81%')\n",
      "(('noun.group', 'noun.communication'), '2.31%')\n",
      "(('noun.substance', 'noun.artifact'), '2.31%')\n",
      "(('noun.quantity', 'noun.person'), '2.11%')\n",
      "(('noun.quantity', 'noun.act'), '2.01%')\n",
      "(('noun.quantity', 'noun.artifact'), '11.43%')\n",
      "(('noun.quantity', 'noun.attribute'), '1.81%')\n",
      "(('noun.person', 'noun.cognition'), '1.81%')\n",
      "(('noun.quantity', 'noun.food'), '1.71%')\n",
      "(('noun.person', 'noun.communication'), '1.40%')\n",
      "(('noun.group', 'noun.cognition'), '1.40%')\n",
      "(('noun.person', 'noun.attribute'), '1.30%')\n",
      "(('noun.quantity', 'noun.group'), '1.20%')\n",
      "(('noun.group', 'noun.possession'), '1.00%')\n",
      "(('noun.person', 'noun.phenomenon'), '1.00%')\n",
      "(('noun.quantity', 'noun.phenomenon'), '1.00%')\n",
      "(('noun.communication', 'noun.attribute'), '1.00%')\n",
      "(('noun.artifact', 'noun.artifact'), '0.90%')\n",
      "(('noun.quantity', 'noun.quantity'), '0.90%')\n",
      "(('noun.group', 'noun.food'), '0.90%')\n",
      "(('noun.group', 'noun.attribute'), '0.90%')\n",
      "(('noun.cognition', 'noun.artifact'), '0.90%')\n",
      "(('noun.group', 'noun.phenomenon'), '0.80%')\n",
      "(('noun.substance', 'noun.communication'), '0.80%')\n",
      "(('noun.location', 'noun.artifact'), '0.80%')\n",
      "(('noun.group', 'noun.group'), '0.80%')\n",
      "(('noun.quantity', 'noun.substance'), '0.80%')\n",
      "(('noun.quantity', 'noun.possession'), '0.80%')\n",
      "(('noun.act', 'noun.artifact'), '0.70%')\n",
      "(('noun.group', 'noun.person'), '0.70%')\n",
      "(('noun.communication', 'noun.artifact'), '0.70%')\n",
      "(('noun.substance', 'noun.substance'), '0.70%')\n",
      "(('noun.person', 'noun.person'), '0.70%')\n",
      "(('noun.group', 'noun.act'), '0.70%')\n",
      "(('noun.possession', 'noun.communication'), '0.60%')\n",
      "(('noun.artifact', 'noun.cognition'), '0.60%')\n",
      "(('noun.artifact', 'noun.possession'), '0.60%')\n",
      "(('noun.possession', 'noun.feeling'), '0.60%')\n",
      "(('noun.quantity', 'noun.location'), '0.60%')\n",
      "(('noun.artifact', 'noun.substance'), '0.60%')\n",
      "(('noun.group', 'noun.plant'), '0.60%')\n",
      "(('noun.person', 'noun.possession'), '0.50%')\n",
      "(('noun.person', 'noun.animal'), '0.50%')\n",
      "(('noun.substance', 'noun.cognition'), '0.50%')\n",
      "(('noun.person', 'noun.time'), '0.50%')\n",
      "(('noun.state', 'noun.artifact'), '0.50%')\n",
      "(('noun.Tops', 'noun.artifact'), '0.50%')\n",
      "(('noun.communication', 'noun.cognition'), '0.50%')\n",
      "(('noun.person', 'noun.group'), '0.50%')\n",
      "(('noun.quantity', 'noun.Tops'), '0.50%')\n",
      "(('noun.quantity', 'noun.plant'), '0.50%')\n",
      "(('noun.attribute', 'noun.artifact'), '0.50%')\n",
      "(('noun.artifact', 'noun.phenomenon'), '0.50%')\n",
      "(('noun.substance', 'noun.possession'), '0.50%')\n",
      "(('noun.quantity', 'noun.body'), '0.40%')\n",
      "(('noun.person', 'noun.food'), '0.40%')\n",
      "(('noun.communication', 'noun.communication'), '0.40%')\n",
      "(('noun.substance', 'noun.person'), '0.40%')\n",
      "(('noun.group', 'noun.Tops'), '0.40%')\n",
      "(('noun.cognition', 'noun.cognition'), '0.40%')\n",
      "(('noun.group', 'noun.location'), '0.40%')\n",
      "(('noun.cognition', 'noun.communication'), '0.30%')\n",
      "(('noun.quantity', 'noun.state'), '0.30%')\n",
      "(('noun.cognition', 'noun.group'), '0.30%')\n",
      "(('noun.Tops', 'noun.communication'), '0.30%')\n",
      "(('noun.quantity', 'noun.animal'), '0.30%')\n",
      "(('noun.act', 'noun.person'), '0.30%')\n",
      "(('noun.person', 'noun.quantity'), '0.30%')\n",
      "(('noun.quantity', 'noun.object'), '0.30%')\n",
      "(('noun.location', 'noun.communication'), '0.30%')\n",
      "(('noun.substance', 'noun.food'), '0.30%')\n",
      "(('noun.person', 'noun.plant'), '0.30%')\n",
      "(('noun.quantity', 'noun.process'), '0.30%')\n",
      "(('noun.cognition', 'noun.substance'), '0.30%')\n",
      "(('noun.group', 'noun.body'), '0.30%')\n",
      "(('noun.communication', 'noun.act'), '0.30%')\n",
      "(('noun.state', 'noun.act'), '0.20%')\n",
      "(('noun.artifact', 'noun.quantity'), '0.20%')\n",
      "(('noun.attribute', 'noun.act'), '0.20%')\n",
      "(('noun.person', 'noun.act'), '0.20%')\n",
      "(('noun.object', 'noun.artifact'), '0.20%')\n",
      "(('noun.substance', 'noun.attribute'), '0.20%')\n",
      "(('noun.location', 'noun.act'), '0.20%')\n",
      "(('noun.person', 'noun.substance'), '0.20%')\n",
      "(('noun.location', 'noun.Tops'), '0.20%')\n",
      "(('noun.substance', 'noun.act'), '0.20%')\n",
      "(('noun.artifact', 'noun.state'), '0.20%')\n",
      "(('noun.person', 'noun.object'), '0.20%')\n",
      "(('noun.artifact', 'noun.group'), '0.20%')\n",
      "(('noun.body', 'noun.person'), '0.20%')\n",
      "(('noun.artifact', 'noun.communication'), '0.20%')\n",
      "(('noun.quantity', 'noun.event'), '0.20%')\n",
      "(('noun.communication', 'noun.group'), '0.20%')\n",
      "(('noun.cognition', 'noun.possession'), '0.20%')\n",
      "(('noun.cognition', 'noun.person'), '0.20%')\n",
      "(('noun.person', 'noun.state'), '0.20%')\n",
      "(('noun.group', 'noun.substance'), '0.20%')\n",
      "(('noun.substance', 'noun.animal'), '0.20%')\n",
      "(('noun.location', 'noun.phenomenon'), '0.20%')\n",
      "(('noun.group', 'noun.time'), '0.20%')\n",
      "(('noun.possession', 'noun.cognition'), '0.20%')\n",
      "(('noun.possession', 'noun.artifact'), '0.20%')\n",
      "(('noun.animal', 'noun.artifact'), '0.20%')\n",
      "(('noun.substance', 'noun.relation'), '0.20%')\n",
      "(('noun.artifact', 'noun.food'), '0.10%')\n",
      "(('noun.state', 'noun.cognition'), '0.10%')\n",
      "(('noun.cognition', 'noun.quantity'), '0.10%')\n",
      "(('noun.relation', 'noun.body'), '0.10%')\n",
      "(('noun.substance', 'noun.group'), '0.10%')\n",
      "(('noun.possession', 'noun.act'), '0.10%')\n",
      "(('noun.process', 'noun.artifact'), '0.10%')\n",
      "(('noun.act', 'noun.possession'), '0.10%')\n",
      "(('noun.communication', 'noun.quantity'), '0.10%')\n",
      "(('noun.possession', 'noun.possession'), '0.10%')\n",
      "(('noun.act', 'noun.location'), '0.10%')\n",
      "(('noun.food', 'noun.communication'), '0.10%')\n",
      "(('noun.person', 'noun.Tops'), '0.10%')\n",
      "(('noun.location', 'noun.attribute'), '0.10%')\n",
      "(('noun.substance', 'noun.phenomenon'), '0.10%')\n",
      "(('noun.attribute', 'noun.location'), '0.10%')\n",
      "(('noun.act', 'noun.time'), '0.10%')\n",
      "(('noun.substance', 'noun.object'), '0.10%')\n",
      "(('noun.communication', 'noun.time'), '0.10%')\n",
      "(('noun.time', 'noun.artifact'), '0.10%')\n",
      "(('noun.object', 'noun.body'), '0.10%')\n",
      "(('noun.body', 'noun.artifact'), '0.10%')\n",
      "(('noun.act', 'noun.act'), '0.10%')\n",
      "(('noun.cognition', 'noun.state'), '0.10%')\n",
      "(('noun.artifact', 'noun.animal'), '0.10%')\n",
      "(('noun.cognition', 'noun.attribute'), '0.10%')\n",
      "(('noun.group', 'noun.quantity'), '0.10%')\n",
      "(('noun.Tops', 'noun.time'), '0.10%')\n",
      "(('noun.group', 'noun.event'), '0.10%')\n",
      "(('noun.group', 'noun.state'), '0.10%')\n",
      "(('noun.attribute', 'noun.state'), '0.10%')\n",
      "(('noun.substance', 'noun.quantity'), '0.10%')\n",
      "(('noun.location', 'noun.cognition'), '0.10%')\n",
      "(('noun.shape', 'noun.cognition'), '0.10%')\n",
      "(('noun.phenomenon', 'noun.location'), '0.10%')\n",
      "(('noun.location', 'noun.group'), '0.10%')\n",
      "(('noun.artifact', 'noun.event'), '0.10%')\n",
      "(('noun.attribute', 'noun.substance'), '0.10%')\n",
      "(('noun.animal', 'noun.cognition'), '0.10%')\n",
      "(('noun.act', 'noun.communication'), '0.10%')\n",
      "(('noun.location', 'noun.quantity'), '0.10%')\n",
      "(('noun.act', 'noun.quantity'), '0.10%')\n",
      "(('noun.phenomenon', 'noun.phenomenon'), '0.10%')\n",
      "(('noun.cognition', 'noun.time'), '0.10%')\n",
      "(('noun.substance', 'noun.location'), '0.10%')\n",
      "(('noun.possession', 'noun.attribute'), '0.10%')\n",
      "(('noun.communication', 'noun.animal'), '0.10%')\n",
      "(('noun.animal', 'noun.time'), '0.10%')\n",
      "(('noun.time', 'noun.cognition'), '0.10%')\n",
      "(('noun.artifact', 'noun.plant'), '0.10%')\n",
      "(('noun.location', 'noun.person'), '0.10%')\n",
      "(('noun.cognition', 'noun.act'), '0.10%')\n",
      "(('noun.possession', 'noun.person'), '0.10%')\n",
      "(('noun.quantity', 'noun.time'), '0.10%')\n",
      "(('noun.possession', 'noun.quantity'), '0.10%')\n",
      "(('noun.plant', 'noun.food'), '0.10%')\n",
      "(('noun.artifact', 'noun.act'), '0.10%')\n",
      "(('noun.Tops', 'noun.attribute'), '0.10%')\n",
      "(('noun.location', 'noun.animal'), '0.10%')\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero di pattern trovati: \", len(patterns))\n",
    "# Obtain semantic clusters with frequency \n",
    "semantic_clusters = compute_semantic_clusters(patterns)\n",
    "\n",
    "# Check if probabilities are consistent\n",
    "print(sum(float(semantic_clusters[i][1][0:-1]) for i in range(0, len(semantic_clusters))))\n",
    "\n",
    "# Print clusters\n",
    "for cluster in semantic_clusters:\n",
    "    print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408ddf-bd2c-4d1f-9f82-dfe6fd095bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f530994-ddbf-4c7a-af1d-9587e1ac08df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
