{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75fe4662-5df7-420d-83c7-ae325e1c1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea48f5b-172a-4146-b548-4fb94a9abfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(str(sentence))):\n",
    "        if tag[1][:2] == \"NN\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif tag[1][:2] == \"VB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif tag[1][:2] == \"RB\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif tag[1][:2] == \"JJ\":\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526d97be-0090-4263-b2ff-62b0f959729b",
   "metadata": {},
   "source": [
    "Given an subject, obtain the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e381080f-09d5-4ec3-bcd1-e1ae3b3442bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_obj(sentence, verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == verb and token.dep_ == 'dobj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dep2 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dep2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7943b5-33c7-403f-9db9-55f1ecefeb1a",
   "metadata": {},
   "source": [
    "Given an object, obtain the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f3f446-f0f2-41ee-914d-1ea45c97834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_subj(sentence, verb, pattern):\n",
    "    for token in sentence:\n",
    "        if token.head.text == verb and token.dep_ == 'nsubj' and len(\n",
    "                pattern) < 2 and token.text != '\\n  ' and token.text != '\\n':\n",
    "            dep1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "            return dep1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4b029-c943-4b06-92b0-666195ca4298",
   "metadata": {},
   "source": [
    "Given a pattern like [dep1, dep2, sentence], do WSD using Lesk algorithm. \n",
    "It returns a new pattern with word_super_sense1, word_super_sense2.\n",
    "\n",
    "Pattern example: [('You', 'PRP', 'buy', 'nsubj'), ('products', 'NNS', 'buy', 'dobj'), \n",
    "You can buy our products by PAYPAL Or Credit Card. \n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7656542-725c-4231-a828-84e8f5560a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_terms(pattern):\n",
    "    dep1 = pattern[0]\n",
    "    dep2 = pattern[1]\n",
    "\n",
    "    sentence = pattern[2]\n",
    "\n",
    "    w1 = dep1[0]\n",
    "    w2 = dep2[0]\n",
    "    \n",
    "    # WSD with nltk Lesk\n",
    "    return lesk(tokenize_sentence(sentence), w1, 'n'), lesk(tokenize_sentence(sentence), w2, 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252cb39-0653-46bf-8a76-deaf3b84502c",
   "metadata": {},
   "source": [
    "Calculates semantic clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc99bd3-3621-445d-8e2b-5a5651f8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_clusters(patterns):\n",
    "    new_patterns = []\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        best_sense1, best_sense2 = disambiguate_terms(pattern)\n",
    "        \n",
    "        if best_sense1 and best_sense2:\n",
    "            # supersense1, supersense2\n",
    "            new_patterns.append((best_sense1._lexname, best_sense2._lexname))\n",
    "\n",
    "    # Create a dict of Counter. Res like ((supersense1, supersense2), frequency)\n",
    "    patterns_Counter = dict(Counter(new_patterns))\n",
    "    semantic_clusters = []\n",
    "    \n",
    "    for key in patterns_Counter.keys():\n",
    "        result = patterns_Counter[key]\n",
    "        percentage = result / len(new_patterns)\n",
    "        # Cluster is a grouping of (subject, object) tuples with frequency associated\n",
    "        cluster = key, format(percentage * 100, '.2f') + '%'\n",
    "        semantic_clusters.append(cluster)\n",
    "\n",
    "    # sort clusters by frequency\n",
    "    semantic_clusters = sorted(semantic_clusters, key=lambda x: x[1], reverse=True)\n",
    "    return semantic_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2078c7cf-bdff-4252-9cf2-040498920bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns():\n",
    "    \"\"\"\n",
    "    VERB = \"buy\"\n",
    "    CORPUS = \"buy_corpus.xml\"\n",
    "    all_verb_forms = ['buy', 'buys']\n",
    "    \"\"\"\n",
    "\n",
    "    VERB = \"eat\"\n",
    "    CORPUS = \"eat_corpus.xml\"\n",
    "    all_verb_forms = ['eat', 'eats']\n",
    "    \n",
    "    print(\"Verb: \", VERB)\n",
    "    print(\"Verb forms: \", all_verb_forms)\n",
    "\n",
    "\n",
    "    # Valenza\n",
    "    num_args = 2\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Use minidom to parse XML file\n",
    "    mydoc = minidom.parse(CORPUS)\n",
    "\n",
    "    # Obtain all sentences \n",
    "    sentences = mydoc.getElementsByTagName('line')\n",
    "\n",
    "    patterns = []\n",
    "    for sentence in sentences:\n",
    "        # nlp do dependecies parsing and PoS\n",
    "        sentence_parsed = nlp(sentence.firstChild.data.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"     \", \"\"))\n",
    "        pattern = []\n",
    "\n",
    "        for token in sentence_parsed:\n",
    "            # if token is a subject we can find subjects\n",
    "            if token.head.text in all_verb_forms and token.dep_ == 'nsubj' and len(\n",
    "                    pattern) < num_args and token.text != '\\n  ' and token.text != '\\n':\n",
    "                dep1 = token.text, token.tag_, token.head.text, token.dep_\n",
    "                pattern.append(dep1)\n",
    "                dep2 = search_obj(sentence_parsed, token.head.text, pattern)\n",
    "                if dep2:\n",
    "                    pattern.append(dep2)\n",
    "        if len(pattern) == num_args:  # append iff we have 2 arguments\n",
    "            pattern.append(sentence_parsed)\n",
    "            patterns.append(pattern)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7577133-86e6-4682-8088-788ffe353da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb:  eat\n",
      "Verb forms:  ['eat', 'eats']\n",
      "Patterns found:  2921\n",
      "\n",
      "Cluster list with probability: \n",
      "(('noun.quantity', 'noun.food'), '8.32%')\n",
      "(('noun.quantity', 'noun.cognition'), '5.52%')\n",
      "(('noun.group', 'noun.food'), '4.84%')\n",
      "(('noun.person', 'noun.food'), '4.16%')\n",
      "(('noun.quantity', 'noun.quantity'), '2.72%')\n",
      "(('noun.quantity', 'noun.artifact'), '2.72%')\n",
      "(('noun.substance', 'noun.food'), '2.29%')\n",
      "(('noun.group', 'noun.cognition'), '2.04%')\n",
      "(('noun.person', 'noun.artifact'), '1.61%')\n",
      "(('noun.person', 'noun.cognition'), '1.53%')\n",
      "(('noun.group', 'noun.act'), '1.53%')\n",
      "(('noun.substance', 'noun.cognition'), '1.27%')\n",
      "(('noun.group', 'noun.group'), '1.27%')\n",
      "(('noun.quantity', 'noun.time'), '1.27%')\n",
      "(('noun.substance', 'noun.quantity'), '1.19%')\n",
      "(('noun.quantity', 'noun.Tops'), '1.19%')\n",
      "(('noun.group', 'noun.substance'), '1.10%')\n",
      "(('noun.group', 'noun.plant'), '1.10%')\n",
      "(('noun.group', 'noun.artifact'), '1.10%')\n",
      "(('noun.quantity', 'noun.person'), '1.10%')\n",
      "(('noun.cognition', 'noun.cognition'), '1.02%')\n",
      "(('noun.animal', 'noun.animal'), '0.93%')\n",
      "(('noun.person', 'noun.person'), '0.93%')\n",
      "(('noun.quantity', 'noun.plant'), '0.93%')\n",
      "(('noun.person', 'noun.animal'), '0.93%')\n",
      "(('noun.substance', 'noun.substance'), '0.93%')\n",
      "(('noun.quantity', 'noun.group'), '0.93%')\n",
      "(('noun.artifact', 'noun.food'), '0.85%')\n",
      "(('noun.person', 'noun.plant'), '0.85%')\n",
      "(('noun.animal', 'noun.food'), '0.85%')\n",
      "(('noun.artifact', 'noun.cognition'), '0.85%')\n",
      "(('noun.quantity', 'noun.attribute'), '0.85%')\n",
      "(('noun.animal', 'noun.cognition'), '0.76%')\n",
      "(('noun.animal', 'noun.communication'), '0.76%')\n",
      "(('noun.person', 'noun.act'), '0.76%')\n",
      "(('noun.group', 'noun.person'), '0.76%')\n",
      "(('noun.animal', 'noun.artifact'), '0.68%')\n",
      "(('noun.substance', 'noun.person'), '0.68%')\n",
      "(('noun.cognition', 'noun.artifact'), '0.68%')\n",
      "(('noun.substance', 'noun.artifact'), '0.68%')\n",
      "(('noun.person', 'noun.quantity'), '0.59%')\n",
      "(('noun.Tops', 'noun.food'), '0.59%')\n",
      "(('noun.group', 'noun.quantity'), '0.59%')\n",
      "(('noun.Tops', 'noun.cognition'), '0.59%')\n",
      "(('noun.quantity', 'noun.substance'), '0.59%')\n",
      "(('noun.person', 'noun.Tops'), '0.59%')\n",
      "(('noun.artifact', 'noun.artifact'), '0.59%')\n",
      "(('noun.animal', 'noun.quantity'), '0.59%')\n",
      "(('noun.quantity', 'noun.possession'), '0.51%')\n",
      "(('noun.cognition', 'noun.communication'), '0.51%')\n",
      "(('noun.quantity', 'noun.act'), '0.51%')\n",
      "(('noun.cognition', 'noun.food'), '0.51%')\n",
      "(('noun.artifact', 'noun.animal'), '0.51%')\n",
      "(('noun.group', 'noun.possession'), '0.51%')\n",
      "(('noun.quantity', 'noun.state'), '0.51%')\n",
      "(('noun.location', 'noun.communication'), '0.51%')\n",
      "(('noun.person', 'noun.group'), '0.51%')\n",
      "(('noun.substance', 'noun.plant'), '0.51%')\n",
      "(('noun.person', 'noun.communication'), '0.42%')\n",
      "(('noun.group', 'noun.Tops'), '0.42%')\n",
      "(('noun.group', 'noun.body'), '0.42%')\n",
      "(('noun.substance', 'noun.group'), '0.42%')\n",
      "(('noun.food', 'noun.food'), '0.42%')\n",
      "(('noun.quantity', 'noun.communication'), '0.42%')\n",
      "(('noun.cognition', 'noun.act'), '0.42%')\n",
      "(('noun.person', 'noun.possession'), '0.42%')\n",
      "(('noun.communication', 'noun.food'), '0.42%')\n",
      "(('noun.animal', 'noun.body'), '0.42%')\n",
      "(('noun.attribute', 'noun.food'), '0.42%')\n",
      "(('noun.animal', 'noun.plant'), '0.34%')\n",
      "(('noun.cognition', 'noun.possession'), '0.34%')\n",
      "(('noun.group', 'noun.animal'), '0.34%')\n",
      "(('noun.person', 'noun.time'), '0.34%')\n",
      "(('noun.quantity', 'noun.animal'), '0.34%')\n",
      "(('noun.communication', 'noun.cognition'), '0.34%')\n",
      "(('noun.group', 'noun.time'), '0.34%')\n",
      "(('noun.person', 'noun.state'), '0.34%')\n",
      "(('noun.food', 'noun.cognition'), '0.34%')\n",
      "(('noun.person', 'noun.attribute'), '0.34%')\n",
      "(('noun.substance', 'noun.communication'), '0.34%')\n",
      "(('noun.animal', 'noun.person'), '0.34%')\n",
      "(('noun.location', 'noun.cognition'), '0.34%')\n",
      "(('noun.cognition', 'noun.animal'), '0.34%')\n",
      "(('noun.artifact', 'noun.quantity'), '0.34%')\n",
      "(('noun.quantity', 'noun.process'), '0.34%')\n",
      "(('noun.process', 'noun.artifact'), '0.25%')\n",
      "(('noun.person', 'noun.body'), '0.25%')\n",
      "(('noun.animal', 'noun.relation'), '0.25%')\n",
      "(('noun.person', 'noun.phenomenon'), '0.25%')\n",
      "(('noun.location', 'noun.food'), '0.25%')\n",
      "(('noun.substance', 'noun.attribute'), '0.25%')\n",
      "(('noun.animal', 'noun.act'), '0.25%')\n",
      "(('noun.substance', 'noun.Tops'), '0.25%')\n",
      "(('noun.location', 'noun.quantity'), '0.25%')\n",
      "(('noun.artifact', 'noun.group'), '0.25%')\n",
      "(('noun.quantity', 'noun.body'), '0.25%')\n",
      "(('noun.artifact', 'noun.plant'), '0.25%')\n",
      "(('noun.act', 'noun.group'), '0.25%')\n",
      "(('noun.cognition', 'noun.group'), '0.25%')\n",
      "(('noun.person', 'noun.substance'), '0.25%')\n",
      "(('noun.communication', 'noun.substance'), '0.25%')\n",
      "(('noun.artifact', 'noun.process'), '0.17%')\n",
      "(('noun.cognition', 'noun.Tops'), '0.17%')\n",
      "(('noun.group', 'noun.phenomenon'), '0.17%')\n",
      "(('noun.animal', 'noun.attribute'), '0.17%')\n",
      "(('noun.act', 'noun.body'), '0.17%')\n",
      "(('noun.artifact', 'noun.communication'), '0.17%')\n",
      "(('noun.quantity', 'noun.object'), '0.17%')\n",
      "(('noun.food', 'noun.substance'), '0.17%')\n",
      "(('noun.group', 'noun.communication'), '0.17%')\n",
      "(('noun.location', 'noun.person'), '0.17%')\n",
      "(('noun.state', 'noun.food'), '0.17%')\n",
      "(('noun.location', 'noun.group'), '0.17%')\n",
      "(('noun.possession', 'noun.possession'), '0.17%')\n",
      "(('noun.cognition', 'noun.state'), '0.17%')\n",
      "(('noun.animal', 'noun.state'), '0.17%')\n",
      "(('noun.communication', 'noun.plant'), '0.17%')\n",
      "(('noun.substance', 'noun.act'), '0.17%')\n",
      "(('noun.communication', 'noun.quantity'), '0.17%')\n",
      "(('noun.group', 'noun.state'), '0.17%')\n",
      "(('noun.location', 'noun.plant'), '0.17%')\n",
      "(('noun.cognition', 'noun.person'), '0.17%')\n",
      "(('noun.Tops', 'noun.artifact'), '0.17%')\n",
      "(('noun.cognition', 'noun.time'), '0.17%')\n",
      "(('noun.substance', 'noun.time'), '0.17%')\n",
      "(('noun.communication', 'noun.person'), '0.17%')\n",
      "(('noun.group', 'noun.attribute'), '0.17%')\n",
      "(('noun.event', 'noun.artifact'), '0.17%')\n",
      "(('noun.animal', 'noun.location'), '0.17%')\n",
      "(('noun.state', 'noun.artifact'), '0.17%')\n",
      "(('noun.substance', 'noun.possession'), '0.17%')\n",
      "(('noun.person', 'noun.location'), '0.17%')\n",
      "(('noun.attribute', 'noun.person'), '0.17%')\n",
      "(('noun.location', 'noun.state'), '0.17%')\n",
      "(('noun.communication', 'noun.body'), '0.17%')\n",
      "(('noun.quantity', 'noun.location'), '0.08%')\n",
      "(('noun.body', 'noun.group'), '0.08%')\n",
      "(('noun.communication', 'noun.Tops'), '0.08%')\n",
      "(('noun.animal', 'noun.substance'), '0.08%')\n",
      "(('noun.food', 'noun.group'), '0.08%')\n",
      "(('noun.event', 'noun.Tops'), '0.08%')\n",
      "(('noun.state', 'noun.cognition'), '0.08%')\n",
      "(('noun.artifact', 'noun.possession'), '0.08%')\n",
      "(('noun.person', 'noun.process'), '0.08%')\n",
      "(('noun.substance', 'noun.event'), '0.08%')\n",
      "(('noun.person', 'noun.shape'), '0.08%')\n",
      "(('noun.artifact', 'noun.act'), '0.08%')\n",
      "(('noun.animal', 'noun.process'), '0.08%')\n",
      "(('noun.state', 'noun.act'), '0.08%')\n",
      "(('noun.location', 'noun.shape'), '0.08%')\n",
      "(('noun.location', 'noun.body'), '0.08%')\n",
      "(('noun.possession', 'noun.act'), '0.08%')\n",
      "(('noun.act', 'noun.possession'), '0.08%')\n",
      "(('noun.attribute', 'noun.quantity'), '0.08%')\n",
      "(('noun.feeling', 'noun.quantity'), '0.08%')\n",
      "(('noun.substance', 'noun.body'), '0.08%')\n",
      "(('noun.artifact', 'noun.Tops'), '0.08%')\n",
      "(('noun.Tops', 'noun.group'), '0.08%')\n",
      "(('noun.quantity', 'noun.shape'), '0.08%')\n",
      "(('noun.communication', 'noun.animal'), '0.08%')\n",
      "(('noun.cognition', 'noun.quantity'), '0.08%')\n",
      "(('noun.time', 'noun.quantity'), '0.08%')\n",
      "(('noun.body', 'noun.animal'), '0.08%')\n",
      "(('noun.body', 'noun.state'), '0.08%')\n",
      "(('noun.act', 'noun.food'), '0.08%')\n",
      "(('noun.act', 'noun.relation'), '0.08%')\n",
      "(('noun.act', 'noun.quantity'), '0.08%')\n",
      "(('noun.object', 'noun.food'), '0.08%')\n",
      "(('noun.food', 'noun.plant'), '0.08%')\n",
      "(('noun.food', 'noun.animal'), '0.08%')\n",
      "(('noun.artifact', 'noun.attribute'), '0.08%')\n",
      "(('noun.cognition', 'noun.body'), '0.08%')\n",
      "(('noun.communication', 'noun.location'), '0.08%')\n",
      "(('noun.food', 'noun.artifact'), '0.08%')\n",
      "(('noun.body', 'noun.cognition'), '0.08%')\n",
      "(('noun.Tops', 'noun.possession'), '0.08%')\n",
      "(('noun.act', 'noun.artifact'), '0.08%')\n",
      "(('noun.artifact', 'noun.object'), '0.08%')\n",
      "(('noun.person', 'noun.relation'), '0.08%')\n",
      "(('noun.artifact', 'noun.event'), '0.08%')\n",
      "(('noun.artifact', 'noun.person'), '0.08%')\n",
      "(('noun.communication', 'noun.communication'), '0.08%')\n",
      "(('noun.state', 'noun.person'), '0.08%')\n",
      "(('noun.attribute', 'noun.Tops'), '0.08%')\n",
      "(('noun.communication', 'noun.attribute'), '0.08%')\n",
      "(('noun.artifact', 'noun.body'), '0.08%')\n",
      "(('noun.event', 'noun.group'), '0.08%')\n",
      "(('noun.act', 'noun.shape'), '0.08%')\n",
      "(('noun.phenomenon', 'noun.food'), '0.08%')\n",
      "(('noun.cognition', 'noun.location'), '0.08%')\n",
      "(('noun.animal', 'noun.feeling'), '0.08%')\n",
      "(('noun.possession', 'noun.quantity'), '0.08%')\n",
      "(('noun.body', 'noun.artifact'), '0.08%')\n",
      "(('noun.artifact', 'noun.phenomenon'), '0.08%')\n",
      "(('noun.quantity', 'noun.phenomenon'), '0.08%')\n",
      "(('noun.location', 'noun.artifact'), '0.08%')\n",
      "(('noun.animal', 'noun.time'), '0.08%')\n",
      "(('noun.relation', 'noun.event'), '0.08%')\n",
      "(('noun.possession', 'noun.artifact'), '0.08%')\n",
      "(('noun.location', 'noun.act'), '0.08%')\n",
      "(('noun.location', 'noun.possession'), '0.08%')\n",
      "(('noun.animal', 'noun.Tops'), '0.08%')\n",
      "\n",
      "Sum of all probabilities:  99.50000000000011\n"
     ]
    }
   ],
   "source": [
    "patterns = find_patterns()\n",
    "\n",
    "print(\"Patterns found: \", len(patterns))\n",
    "# Obtain semantic clusters with frequency \n",
    "semantic_clusters = semantic_clusters(patterns)\n",
    "\n",
    "# Print clusters\n",
    "print('\\nCluster list with probability: ')\n",
    "for cluster in semantic_clusters:\n",
    "    print(cluster)\n",
    "    \n",
    "# Check if probabilities are consistent\n",
    "print('\\nSum of all probabilities: ',sum(float(semantic_clusters[i][1][0:-1]) for i in range(0, len(semantic_clusters))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8f963-599b-4730-b0b4-7901446df0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a99f29-81f5-44ee-a516-fbb9eb9fe85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
