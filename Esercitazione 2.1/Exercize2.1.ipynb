{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc20e482-4a22-4d78-84a4-58a62bfabfc9",
   "metadata": {},
   "source": [
    "## Exercize 2.1\n",
    "### Summarization using Nasari\n",
    "#### Francesco Sannicola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6485cd96-b8a1-45b5-bb0d-ac4228a302d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4546dcb-58e2-4b98-bad9-4dce0a01276f",
   "metadata": {},
   "source": [
    "Reads the NASARI file and calculate a dict as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e56a58-c1a5-46c0-90b1-0f8b173dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nasari(file):\n",
    "    \"\"\"\n",
    "    :return: a dictionary as {word: {{term:score},...}}\n",
    "    \"\"\"\n",
    "    nasari = {}\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            line_splitted = row.split(\";\")\n",
    "            dict_entry = {}\n",
    "            \n",
    "            # Start from 2 letter (delete \"bn:\")\n",
    "            for term in line_splitted[2:]:\n",
    "                # term and score written like this: \"serotonin_1841.0\"\n",
    "                term_score = term.split(\"_\")\n",
    "                if len(term_score) > 1:\n",
    "                    dict_entry[term_score[0]] = term_score[1]\n",
    "\n",
    "            nasari[line_splitted[1].lower()] = dict_entry\n",
    "\n",
    "    return nasari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5524a05-467b-41fb-be83-60af0aee0b21",
   "metadata": {},
   "source": [
    "Reads the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f42c7b-d0cb-413a-95f2-804b4bdc3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(file):\n",
    "    \"\"\"\n",
    "    :param file: document's path\n",
    "    :return: a list of all paragraph.\n",
    "    \"\"\"\n",
    "    document = []\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            # does not consider lines starting with \"#\"\n",
    "            if '#' not in row:\n",
    "                row = row[:-1]\n",
    "                if row != '':\n",
    "                    document.append(row)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c5eb3-132b-4f8e-90fe-aad27ba121ac",
   "metadata": {},
   "source": [
    "Computes the rank of the given vector. Method used to calculate the weighted overlap between nasari vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd63962-b056-40b1-8e60-6ae3a5a3d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(vector, nasari_vector):\n",
    "    \"\"\"\n",
    "    :param vector: input vector\n",
    "    :param nasari_vector: input Nasari vector\n",
    "    :return: vector's rank (position inside the nasari_vector)\n",
    "    \"\"\"\n",
    "\n",
    "    j=9\n",
    "    for i in range(len(nasari_vector)):\n",
    "        if nasari_vector[i] == vector:\n",
    "            # returns index of nasari_vector egual to input vector\n",
    "            return i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74a2b3-11d6-4e0d-b9b3-e867c8da0ffc",
   "metadata": {},
   "source": [
    "Implementation of the Weighted Overlap between two nasari vectors.\n",
    "$$\n",
    "  WO(w_1,w_2) = \\frac{\\sum_{q \\in O} (rank(q, v1) + rank(q, v2))^{-1}}{\\sum_{i=1}^{|O|} (2i)^{-1}}\n",
    "$$\n",
    "\n",
    "More is WO and more will be similar that 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdef25fe-2e8a-4dce-8557-d94a588b5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_overlap(nasari_vector_1, nasari_vector_2):\n",
    "    \"\"\"\n",
    "    :param nasari_vector_1: in our case represents the Nasari of topic\n",
    "    :param nasari_vector_2: in our case represents the Nasari of paragraph\n",
    "    :return: Weighted Overlap square-rooted of two nasari vectors\n",
    "    \"\"\"\n",
    "\n",
    "    overlap_keys = nasari_vector_1.keys() & nasari_vector_2.keys()\n",
    "\n",
    "    list_overlap_keys = list(overlap_keys)\n",
    "\n",
    "    if len(list_overlap_keys) != 0:\n",
    "        return (sum(1 / (calculate_rank(vector, list(nasari_vector_1)) + calculate_rank(vector, list(nasari_vector_2))) for vector in list_overlap_keys)) \\\n",
    "               / \\\n",
    "               (sum(list(map(lambda x: 1 / (2 * x), list(range(1, len(list_overlap_keys) + 1))))))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fea36-6b13-4650-9890-997e93f9ad28",
   "metadata": {},
   "source": [
    "A bag of word algorithm based approach. It calculates a list of word given a text doing stop word and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97946242-12b9-4e0f-a4dd-6698e94c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_word_approach(text):\n",
    "    \"\"\"\n",
    "    :param text: input text\n",
    "    :return: BoW representation of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    # text tokenizzation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stop_word and punctuation\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in string.punctuation, tokens))\n",
    "    \n",
    "    return set(wordnet_lemmatizer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711ac31-8d40-4435-b4a8-a7a55bdc6c72",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending on the document title (topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4a4590-8f41-4492-8fef-c1c027ed6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_from_title(document, nasari):\n",
    "    \"\"\"\n",
    "    :param document: input document\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: a list of Nasari vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # title in on the first document entry\n",
    "    title = document[0]\n",
    "    \n",
    "    # topic calculated with BOW approach\n",
    "    topic = bag_of_word_approach(title)\n",
    "\n",
    "    # NB nasari_vectors is a dict of dicts {word: {{term:score},...}}\n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in topic:\n",
    "        if word in nasari.keys():\n",
    "            nasari_vectors.append(nasari[word])\n",
    "\n",
    "    return nasari_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24591d8e-d92d-468d-93fa-7844e531065a",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending of text's terms. Very similar to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2769748-b5fb-4959-a9f9-52a4f0cd5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_nasari(text, nasari):\n",
    "    \"\"\"\n",
    "    :param text: the list of text's terms\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: list of Nasari's vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = bag_of_word_approach(text)\n",
    "    \n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in nasari.keys():\n",
    "            nasari_vectors.append(nasari[word]) \n",
    "\n",
    "    return nasari_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb390aff-d337-4e5f-8335-e1b3086fda61",
   "metadata": {},
   "source": [
    "Given a list of paragraph from a document, calculate how many of these are preserved depending to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43a5084f-feb9-49c5-8c11-d69688207bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lines_to_keep(doc_paragraphs, percentage):\n",
    "    \"\"\"\n",
    "    :param doc_paragraphs: document's paragraphs as a list\n",
    "    :param percentage: reduction percentage\n",
    "    :return: number of paragraphs to preserve\n",
    "    \"\"\"\n",
    "    return len(doc_paragraphs) - int(round((percentage / 100) * len(doc_paragraphs), 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fc2a0-1860-4a19-94d0-3d3918d7a50e",
   "metadata": {},
   "source": [
    "Given a list of paragraphs annotated with overlap scores, compute the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c392e1-344e-40ea-a3f1-0aac863fa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_document(doc_paragraphs_overlaps, lines_to_keep):\n",
    "    \"\"\"\n",
    "    :param doc_paragraphs_overlaps: document's paragraphs as a list with an overlap score\n",
    "    :param lines_to_keep: number of paragraph to keep\n",
    "    :return: reduced document\n",
    "    \"\"\"\n",
    "    # Order by weighted overlap\n",
    "    document_sorted  = sorted(doc_paragraphs_overlaps, key=lambda x: x[1], reverse=True)\n",
    "    reduced_document = document_sorted[:lines_to_keep]\n",
    "    \n",
    "    # Restore original order\n",
    "    reduced_document = sorted(reduced_document, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Obtain the text\n",
    "    reduced_document_text = list(map(lambda x: x[2], reduced_document))\n",
    "    \n",
    "    # Add the title\n",
    "    reduced_document_text = [document[0]] + reduced_document_text\n",
    "    \n",
    "    return reduced_document_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bdc07-454e-4868-9e28-39f84435602e",
   "metadata": {},
   "source": [
    "Applies summarization to the given document, with the specific redution percentage.\n",
    "I will use title approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c88a73-1963-4a61-ab85-b40a2011255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(document, nasari, percentage):\n",
    "    \"\"\"\n",
    "    :param document: document\n",
    "    :param nasari_dict: nasari dict of dicts\n",
    "    :param percentage: reduction percentage\n",
    "    :return: document summarized\n",
    "    \"\"\"\n",
    "\n",
    "    # First step: calculate topic/topics from title\n",
    "    topics = get_topic_from_title(document, nasari)\n",
    "    doc_paragraphs = []\n",
    "    i = 0\n",
    "    \n",
    "    for doc_paragraph in document[1:]:\n",
    "        \n",
    "        # obtain nasari rappresentation of the paragraph\n",
    "        nasari_text_par = text_to_nasari(doc_paragraph, nasari)\n",
    "        paragraph_weighted_overlap = 0\n",
    "        \n",
    "        # word is a nasari rappresentation of the term {word: {{term:score},...}}\n",
    "        for word in nasari_text_par:\n",
    "            topic_weighted_overlap = 0\n",
    "            \n",
    "            for topic in topics:\n",
    "                # for each topic compute the WO for topic and word (comulative)\n",
    "                topic_weighted_overlap += weighted_overlap(word, topic)\n",
    "            \n",
    "            # Mean of WO (based on number of topic)\n",
    "            if topic_weighted_overlap != 0:\n",
    "                topic_weighted_overlap /= len(topics)\n",
    "            \n",
    "            \n",
    "            # Comulative paragraph's WO\n",
    "            paragraph_weighted_overlap += topic_weighted_overlap\n",
    "\n",
    "        if len(nasari_text_par) != 0:\n",
    "            # Mean of paragraph's WO\n",
    "            paragraph_weighted_overlap /= len(nasari_text_par)\n",
    "            # Create a tuple with paragraph's number, WO and text. Append it.\n",
    "            doc_paragraphs.append((i, paragraph_weighted_overlap, doc_paragraph))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Obtain number of lines to keep\n",
    "    lines_to_keep = calculate_lines_to_keep(doc_paragraphs, percentage)\n",
    "    \n",
    "    # Finally we can execute summarization\n",
    "    reduced_document = reduce_document(doc_paragraphs, lines_to_keep)\n",
    "    \n",
    "    return reduced_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ae58c-fa26-4b3d-a760-8e3412f3cc8e",
   "metadata": {},
   "source": [
    "Call all previously defined methods.\n",
    "Also calcule BLEU and ROUGE score to see how similar the results are compared to the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bad6ba3-4a29-4576-b6ac-f57eb7577199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "\u001b[1mAndy-Warhol \u001b[0m\tOriginal lenght: 20\n",
      "10 % redution \tSummary lenght: 18\n",
      "\n",
      "BLEU score:  0.8948393168143697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/francesco/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/francesco/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue scores:  [{'rouge-1': {'f': 0.9290555756849036, 'p': 1.0, 'r': 0.8675105485232067}, 'rouge-2': {'f': 0.9136137444598561, 'p': 0.9834469328140214, 'r': 0.8530405405405406}, 'rouge-l': {'f': 0.9369369319568218, 'p': 1.0, 'r': 0.8813559322033898}}]\n",
      "20 % redution \tSummary lenght: 16\n",
      "\n",
      "BLEU score:  0.7788007830714049\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.8523002372398267, 'p': 1.0, 'r': 0.7426160337552743}, 'rouge-2': {'f': 0.8376151187156861, 'p': 0.9829351535836177, 'r': 0.7297297297297297}, 'rouge-l': {'f': 0.8771626248332306, 'p': 1.0, 'r': 0.7812018489984591}}]\n",
      "30 % redution \tSummary lenght: 14\n",
      "\n",
      "BLEU score:  0.6514390575310556\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.8030302982242884, 'p': 1.0, 'r': 0.6708860759493671}, 'rouge-2': {'f': 0.7896865472671786, 'p': 0.9836272040302267, 'r': 0.6596283783783784}, 'rouge-l': {'f': 0.8400357413299089, 'p': 1.0, 'r': 0.724191063174114}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mEbola-virus-disease \u001b[0m\tOriginal lenght: 28\n",
      "10 % redution \tSummary lenght: 23\n",
      "\n",
      "BLEU score:  0.8046150583253528\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9771622884914319, 'p': 1.0, 'r': 0.9553444180522566}, 'rouge-2': {'f': 0.9669421437629409, 'p': 0.9895522388059701, 'r': 0.9453422053231939}, 'rouge-l': {'f': 0.9789719576190334, 'p': 1.0, 'r': 0.9588100686498856}}]\n",
      "20 % redution \tSummary lenght: 20\n",
      "\n",
      "BLEU score:  0.6703200460356393\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9333671092861977, 'p': 1.0, 'r': 0.8750593824228029}, 'rouge-2': {'f': 0.9242078530703846, 'p': 0.9902227050516024, 'r': 0.8664448669201521}, 'rouge-l': {'f': 0.9545454495557851, 'p': 1.0, 'r': 0.9130434782608695}}]\n",
      "30 % redution \tSummary lenght: 18\n",
      "\n",
      "BLEU score:  0.5737534207374327\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.8559782559732766, 'p': 1.0, 'r': 0.7482185273159145}, 'rouge-2': {'f': 0.8471995600847921, 'p': 0.9898348157560356, 'r': 0.7404942965779467}, 'rouge-l': {'f': 0.8809218900773, 'p': 1.0, 'r': 0.7871853546910755}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mLife-indoors \u001b[0m\tOriginal lenght: 13\n",
      "10 % redution \tSummary lenght: 12\n",
      "\n",
      "BLEU score:  0.9200444146293233\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9746835393070021, 'p': 1.0, 'r': 0.9506172839506173}, 'rouge-2': {'f': 0.9583333283365496, 'p': 0.983271375464684, 'r': 0.9346289752650176}, 'rouge-l': {'f': 0.9768875142630716, 'p': 1.0, 'r': 0.9548192771084337}}]\n",
      "20 % redution \tSummary lenght: 11\n",
      "\n",
      "BLEU score:  0.8337529180751805\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9301886742696511, 'p': 1.0, 'r': 0.8694885361552028}, 'rouge-2': {'f': 0.9130434732853299, 'p': 0.9817073170731707, 'r': 0.8533568904593639}, 'rouge-l': {'f': 0.9409888307430895, 'p': 1.0, 'r': 0.8885542168674698}}]\n",
      "30 % redution \tSummary lenght: 9\n",
      "\n",
      "BLEU score:  0.6411803884299546\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.8025343140967587, 'p': 1.0, 'r': 0.6701940035273368}, 'rouge-2': {'f': 0.787301582497377, 'p': 0.9815303430079155, 'r': 0.657243816254417}, 'rouge-l': {'f': 0.8411867316008029, 'p': 1.0, 'r': 0.7259036144578314}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mNapoleon-wiki \u001b[0m\tOriginal lenght: 18\n",
      "10 % redution \tSummary lenght: 16\n",
      "\n",
      "BLEU score:  0.8824969025845955\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9279999950251577, 'p': 0.9988518943742825, 'r': 0.8665338645418327}, 'rouge-2': {'f': 0.9151094451052969, 'p': 0.9850574712643678, 'r': 0.8544366899302094}, 'rouge-l': {'f': 0.920043806640314, 'p': 0.997624703087886, 'r': 0.8536585365853658}}]\n",
      "20 % redution \tSummary lenght: 15\n",
      "\n",
      "BLEU score:  0.8187307530779819\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.868880130143581, 'p': 0.9987063389391979, 'r': 0.7689243027888446}, 'rouge-2': {'f': 0.8563380232536972, 'p': 0.9844559585492227, 'r': 0.7577268195413759}, 'rouge-l': {'f': 0.8795454496152894, 'p': 0.9974226804123711, 'r': 0.7865853658536586}}]\n",
      "30 % redution \tSummary lenght: 13\n",
      "\n",
      "BLEU score:  0.6807123983233854\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.775137106767451, 'p': 0.9984301412872841, 'r': 0.6334661354581673}, 'rouge-2': {'f': 0.7638804101378206, 'p': 0.9842767295597484, 'r': 0.6241276171485544}, 'rouge-l': {'f': 0.8077388101741556, 'p': 0.9970149253731343, 'r': 0.6788617886178862}}]\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "\u001b[1mTrump-wall \u001b[0m\tOriginal lenght: 63\n",
      "10 % redution \tSummary lenght: 51\n",
      "\n",
      "BLEU score:  0.7903383629814982\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.9649890540870917, 'p': 1.0, 'r': 0.9323467230443975}, 'rouge-2': {'f': 0.9529411714767205, 'p': 0.9875248086192231, 'r': 0.9206978588421887}, 'rouge-l': {'f': 0.9717726709533124, 'p': 0.996141975308642, 'r': 0.9485672299779574}}]\n",
      "20 % redution \tSummary lenght: 46\n",
      "\n",
      "BLEU score:  0.6910347152078063\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.8932436335477715, 'p': 1.0, 'r': 0.8070824524312896}, 'rouge-2': {'f': 0.8809245123185745, 'p': 0.9862430396331477, 'r': 0.795929156753899}, 'rouge-l': {'f': 0.9054487129897099, 'p': 0.9955947136563876, 'r': 0.8302718589272594}}]\n",
      "30 % redution \tSummary lenght: 40\n",
      "\n",
      "BLEU score:  0.5627048688069557\n",
      "Rogue scores:  [{'rouge-1': {'f': 0.7807314274943005, 'p': 1.0, 'r': 0.6403276955602537}, 'rouge-2': {'f': 0.7690572071664148, 'p': 0.9851362510322048, 'r': 0.6307163626751255}, 'rouge-l': {'f': 0.816131825171392, 'p': 0.9957671957671957, 'r': 0.6914033798677444}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from bleu import multi_list_bleu\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "nasari_file = 'utils/NASARI_vectors/dd-small-nasari-15.txt'\n",
    "all_docs = ['utils/docs/Andy-Warhol.txt', 'utils/docs/Ebola-virus-disease.txt', 'utils/docs/Life-indoors.txt', 'utils/docs/Napoleon-wiki.txt', 'utils/docs/Trump-wall.txt']\n",
    "\n",
    "nasari = read_nasari(nasari_file)\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "for doc in all_docs:\n",
    "    \n",
    "    document = read_doc(doc)\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print('\\033[1m' +doc.replace(\"utils/docs/\", \"\").replace(\".txt\", \"\"), '\\033[0m' + \"\\tOriginal lenght:\", len(document))\n",
    "    \n",
    "    for percentage_decrease in [10,20,30]:\n",
    "        \n",
    "        summary = summarization(document, nasari, percentage_decrease)\n",
    "\n",
    "        print(percentage_decrease, \"% redution\", \"\\tSummary lenght:\", len(summary))\n",
    "        print()\n",
    "        \n",
    "        # Compute BLEU only for 1-gram\n",
    "        print(\"BLEU score: \", sentence_bleu([document], summary, weights=(1, 0, 0, 0)))\n",
    "        \n",
    "        # COmpute rouge scores for unigram, bigram and l-gram. F1, precision and recall.\n",
    "        rouge_scores = rouge.get_scores(' '.join(summary), ' '.join(document))\n",
    "        print(\"Rogue scores: \", rouge_scores)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02da04-79f4-404e-a24b-b1dc53f74d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
