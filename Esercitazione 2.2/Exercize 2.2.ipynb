{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3db477-5aaa-4665-837e-b7e500236e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa620f-72b9-42ab-8ed7-30364228ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d1d86-387b-4b38-a1d8-9e12a3c75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(txt_file):\n",
    "    \n",
    "    with open(txt_file, encoding='utf-8') as file:\n",
    "        # for each doc create list of pre-processed words in that doc (list of lists)\n",
    "        documents_words = []\n",
    "\n",
    "        for line in file:\n",
    "            if \"<doc\" in line:  # tag for new doc\n",
    "                document_words = []  # list of words that will be part of the document\n",
    "                while True:\n",
    "                    next_line = file.readline()  # read next line\n",
    "\n",
    "                    # remove unuseful tags\n",
    "                    next_line_proc = next_line.replace(\"<p> \", \"\").replace(\"</p>\\n\", \"\").replace(\"/p\", \"\")\n",
    "\n",
    "                    if \"</doc>\" in next_line:\n",
    "                        break\n",
    "                    # pre-processing steps\n",
    "                    sentence_words = pre_processing(next_line_proc)\n",
    "                    document_words.extend(sentence_words)\n",
    "                documents_words.append(document_words)\n",
    "        file.close()\n",
    "\n",
    "    print(\"Documents number: \", len(documents_words))\n",
    "    \n",
    "    return documents_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732815ad-9ac4-4efc-ba4c-452675e0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modelling using gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "topic_num = 10\n",
    "topic_words_num = 5\n",
    "\n",
    "def topic_modelling(documents_words):\n",
    "    \n",
    "    \n",
    "    # Create a dict with integer keys for all words\n",
    "    dictionary_LDA = corpora.Dictionary(documents_words)\n",
    "\n",
    "    # delete all terms that do NOT appear in at least 3 documents.\n",
    "    #delete all terms that appear in more than 50% of documents (see filter_extremes official doc).\n",
    "    dictionary_LDA.filter_extremes(no_below=3)\n",
    "\n",
    "    # Converts each document into a list of BoW (list of (id_term, term_frequency) for each term in doc)\n",
    "    corpus_idbow_freq = [dictionary_LDA.doc2bow(document_words) for document_words in documents_words]\n",
    "    \n",
    "\n",
    "    lda_model = models.LdaModel(corpus_idbow_freq, num_topics=topic_num, \\\n",
    "                                id2word=dictionary_LDA, \\\n",
    "                                passes=4, alpha=[0.01] * topic_num, \\\n",
    "                                eta=[0.01] * len(dictionary_LDA.keys()))\n",
    "    \n",
    "    return lda_model, corpus_idbow_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115948b8-8e87-43b2-8748-d51d60740723",
   "metadata": {},
   "source": [
    "Show topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657c372c-ce95-44c9-b774-358bd18a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:  100\n",
      "[('lesson', 0.133), ('website', 0.034), ('pronunciation', 0.022), ('life', 0.019), ('activity', 0.018)]\n",
      "[('__________________', 0.02), ('write', 0.017), ('rule', 0.016), ('word', 0.016), ('language', 0.015)]\n",
      "[('happen', 0.018), ('conditionals', 0.018), ('result', 0.017), ('condition', 0.015), ('third', 0.014)]\n",
      "[('journey', 0.023), ('rain', 0.016), ('trip', 0.015), ('tour', 0.015), ('place', 0.011)]\n",
      "[('flight', 0.021), ('journey', 0.019), ('subject', 0.015), ('film', 0.013), ('trip', 0.013)]\n",
      "[('word', 0.034), ('language', 0.018), ('article', 0.017), ('noun', 0.014), ('thing', 0.009)]\n",
      "[('student', 0.038), ('lesson', 0.029), ('worksheet', 0.025), ('language', 0.018), ('online', 0.016)]\n",
      "[('hotel', 0.029), ('book', 0.026), ('holiday', 0.025), ('beach', 0.018), ('money', 0.012)]\n",
      "[('students', 0.063), ('exam', 0.032), ('sb', 0.025), ('book', 0.025), ('speak', 0.024)]\n",
      "[('lesson', 0.023), ('student', 0.015), ('audio', 0.015), ('dailystep', 0.011), ('phrasal', 0.009)]\n"
     ]
    }
   ],
   "source": [
    "documents_words = read_corpus(\"travelling.txt\")\n",
    "\n",
    "model, corpus_idbow_freq = topic_modelling(documents_words)\n",
    "\n",
    "topics = {'Topic_' + str(i): [(token, round(score, 3)) for token, score in model.show_topic(i, topn=topic_words_num)] for i in range(0, model.num_topics)}\n",
    "for key, value in topics.items():\n",
    "    print(topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483113ca-cc75-42b0-989b-db18a485ea1a",
   "metadata": {},
   "source": [
    "Show topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3148513-5772-4920-b54c-322c1335b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents topic list\n",
      "Doc 0 [(6, 0.5333893), (7, 0.46527943)]\n",
      "Doc 1 [(7, 0.80343425), (8, 0.19437215)]\n",
      "Doc 2 [(9, 0.9975737)]\n",
      "Doc 3 [(5, 0.6709842), (7, 0.11149991), (8, 0.2170993)]\n",
      "Doc 4 [(9, 0.99980974)]\n",
      "Doc 5 [(5, 0.99993277)]\n",
      "Doc 6 [(0, 0.9989784)]\n",
      "Doc 7 [(1, 0.6054322), (2, 0.20323488), (6, 0.19078636)]\n",
      "Doc 8 [(7, 0.9999108)]\n",
      "Doc 9 [(2, 0.3870464), (3, 0.02362862), (5, 0.36366206), (7, 0.22543569)]\n",
      "Doc 10 [(2, 0.9998624)]\n",
      "Doc 11 [(2, 0.99970204)]\n",
      "Doc 12 [(6, 0.96266425), (7, 0.036494467)]\n",
      "Doc 13 [(0, 0.13591199), (6, 0.20773683), (8, 0.6560809)]\n",
      "Doc 14 [(0, 0.15984409), (4, 0.030539991), (5, 0.6071806), (8, 0.2020906)]\n",
      "Doc 15 [(1, 0.09429425), (2, 0.12214975), (5, 0.072534434), (8, 0.06488724), (9, 0.64604867)]\n",
      "Doc 16 [(1, 0.077662215), (2, 0.92165446)]\n",
      "Doc 17 [(2, 0.18730727), (7, 0.3487466), (9, 0.46373007)]\n",
      "Doc 18 [(2, 0.999622)]\n",
      "Doc 19 [(2, 0.7645136), (4, 0.23514019)]\n",
      "Doc 20 [(2, 0.99981016)]\n",
      "Doc 21 [(2, 0.9995929)]\n",
      "Doc 22 [(2, 0.99958545)]\n",
      "Doc 23 [(2, 0.99964994)]\n",
      "Doc 24 [(1, 0.9990011)]\n",
      "Doc 25 [(2, 0.99983203)]\n",
      "Doc 26 [(2, 0.98042494), (8, 0.019278735)]\n",
      "Doc 27 [(2, 0.9980476)]\n",
      "Doc 28 [(2, 0.999705)]\n",
      "Doc 29 [(2, 0.99970883)]\n",
      "Doc 30 [(2, 0.48856294), (9, 0.5109612)]\n",
      "Doc 31 [(3, 0.9994413)]\n",
      "Doc 32 [(3, 0.9994413)]\n",
      "Doc 33 [(0, 0.9999013)]\n",
      "Doc 34 [(5, 0.99982214)]\n",
      "Doc 35 [(1, 0.21100794), (2, 0.7886302)]\n",
      "Doc 36 [(5, 0.99961877)]\n",
      "Doc 37 [(2, 0.017441878), (3, 0.98226297)]\n",
      "Doc 38 [(0, 0.09374771), (2, 0.90487504)]\n",
      "Doc 39 [(6, 0.99291885)]\n",
      "Doc 40 [(5, 0.24746688), (7, 0.75195366)]\n",
      "Doc 41 [(2, 0.9987341)]\n",
      "Doc 42 [(7, 0.05588901), (9, 0.94399565)]\n",
      "Doc 43 [(2, 0.25495347), (4, 0.7445135)]\n",
      "Doc 44 [(2, 0.37351847), (5, 0.18358348), (6, 0.44249123)]\n",
      "Doc 45 [(5, 0.9996019)]\n",
      "Doc 46 [(6, 0.99983954)]\n",
      "Doc 47 [(8, 0.9999882)]\n",
      "Doc 48 [(1, 0.99978054)]\n",
      "Doc 49 [(2, 0.9998225)]\n",
      "Doc 50 [(7, 0.99994814)]\n",
      "Doc 51 [(7, 0.99978876)]\n",
      "Doc 52 [(1, 0.5543041), (2, 0.44540605)]\n",
      "Doc 53 [(5, 0.04093086), (9, 0.95891434)]\n",
      "Doc 54 [(6, 0.081581615), (9, 0.91739386)]\n",
      "Doc 55 [(2, 0.25219843), (3, 0.13912943), (4, 0.23727185), (7, 0.3707032)]\n",
      "Doc 56 [(4, 0.9994271)]\n",
      "Doc 57 [(2, 0.34836733), (4, 0.05360452), (5, 0.5141454), (8, 0.045092415), (9, 0.03809676)]\n",
      "Doc 58 [(4, 0.58750033), (5, 0.021794003), (6, 0.15778813), (8, 0.04238689), (9, 0.18374811)]\n",
      "Doc 59 [(2, 0.06216629), (8, 0.93757397)]\n",
      "Doc 60 [(0, 0.5225956), (2, 0.21198829), (4, 0.03789765), (5, 0.22716151)]\n",
      "Doc 61 [(2, 0.18644755), (5, 0.8134969)]\n",
      "Doc 62 [(0, 0.9526397), (4, 0.04677676)]\n",
      "Doc 63 [(7, 0.3389949), (9, 0.6606072)]\n",
      "Doc 64 [(1, 0.04396638), (5, 0.015746666), (8, 0.9400668)]\n",
      "Doc 65 [(2, 0.99679655)]\n",
      "Doc 66 [(4, 0.9987342)]\n",
      "Doc 67 [(0, 0.025578571), (4, 0.95110905), (9, 0.022107359)]\n",
      "Doc 68 [(4, 0.9987342)]\n",
      "Doc 69 [(2, 0.050969023), (8, 0.94867074)]\n",
      "Doc 70 [(2, 0.9645701), (5, 0.034541797)]\n",
      "Doc 71 [(2, 0.17646022), (5, 0.15637366), (6, 0.634234), (9, 0.03256638)]\n",
      "Doc 72 [(0, 0.27050552), (2, 0.3493282), (5, 0.379858)]\n",
      "Doc 73 [(2, 0.34561902), (4, 0.65336955)]\n",
      "Doc 74 [(2, 0.9986174)]\n",
      "Doc 75 [(9, 0.9998093)]\n",
      "Doc 76 [(2, 0.35032424), (5, 0.6495183)]\n",
      "Doc 77 [(1, 0.9209757), (9, 0.07874854)]\n",
      "Doc 78 [(2, 0.99933374)]\n",
      "Doc 79 [(2, 0.9405066), (5, 0.059328154)]\n",
      "Doc 80 [(4, 0.9996553)]\n",
      "Doc 81 [(4, 0.13236238), (7, 0.13443385), (9, 0.7329665)]\n",
      "Doc 82 [(5, 0.47108677), (6, 0.25468493), (7, 0.20297456), (8, 0.07055672)]\n",
      "Doc 83 [(5, 0.4586053), (7, 0.5399934)]\n",
      "Doc 84 [(2, 0.14823261), (7, 0.85157514)]\n",
      "Doc 85 [(5, 0.9986383)]\n",
      "Doc 86 [(2, 0.9996155)]\n",
      "Doc 87 [(2, 0.99954337)]\n",
      "Doc 88 [(5, 0.77548814), (9, 0.2232635)]\n",
      "Doc 89 [(9, 0.99954563)]\n",
      "Doc 90 [(1, 0.21728761), (5, 0.7811464)]\n",
      "Doc 91 [(1, 0.9972808)]\n",
      "Doc 92 [(2, 0.9991674)]\n",
      "Doc 93 [(2, 0.9374127), (5, 0.062417477)]\n",
      "Doc 94 [(2, 0.99881727)]\n",
      "Doc 95 [(2, 0.09811737), (9, 0.90151405)]\n",
      "Doc 96 [(0, 0.042909205), (2, 0.55054086), (9, 0.40621024)]\n",
      "Doc 97 [(2, 0.39810917), (7, 0.4028862), (9, 0.19826838)]\n",
      "Doc 98 [(2, 0.9995929)]\n",
      "Doc 99 [(2, 0.9993028)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Documents topic list\")\n",
    "for i in range (0, len(corpus_idbow_freq)):\n",
    "    print (\"Doc\", i, model[corpus_idbow_freq[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1a5a-a3e6-41b6-a63a-d687fac108e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
