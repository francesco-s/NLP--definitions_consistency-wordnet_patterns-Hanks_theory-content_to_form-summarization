{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3db477-5aaa-4665-837e-b7e500236e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa620f-72b9-42ab-8ed7-30364228ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925cc97-00f4-44de-bbdf-20c5ebee3e01",
   "metadata": {},
   "source": [
    "Read corpus file. It return a list of list of documents and words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d1d86-387b-4b38-a1d8-9e12a3c75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(txt_file):\n",
    "    \n",
    "    with open(txt_file, encoding='utf-8') as file:\n",
    "        # for each doc create list of pre-processed words in that doc (list of lists)\n",
    "        documents_words = []\n",
    "\n",
    "        for line in file:\n",
    "            if \"<doc\" in line:  # tag for new doc\n",
    "                document_words = []  # list of words that will be part of the document\n",
    "                while True:\n",
    "                    next_line = file.readline()  # read next line\n",
    "\n",
    "                    # remove unuseful tags\n",
    "                    next_line_proc = next_line.replace(\"<p> \", \"\").replace(\"</p>\\n\", \"\").replace(\"/p\", \"\")\n",
    "\n",
    "                    if \"</doc>\" in next_line:\n",
    "                        break\n",
    "                    # pre-processing steps\n",
    "                    sentence_words = pre_processing(next_line_proc)\n",
    "                    document_words.extend(sentence_words)\n",
    "                documents_words.append(document_words)\n",
    "        file.close()\n",
    "\n",
    "    print(\"Documents number: \", len(documents_words))\n",
    "    \n",
    "    return documents_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dced5e-4328-426d-a7a4-c0043841b2cb",
   "metadata": {},
   "source": [
    "Topic modelling using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732815ad-9ac4-4efc-ba4c-452675e0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "topic_num = 10\n",
    "topic_words_num = 5\n",
    "\n",
    "def topic_modelling(documents_words):\n",
    "    \n",
    "    \n",
    "    # Create a dict with integer keys for all words\n",
    "    dictionary_LDA = corpora.Dictionary(documents_words)\n",
    "\n",
    "    # delete all terms that do NOT appear in at least 3 documents.\n",
    "    #delete all terms that appear in more than 60% of documents (see filter_extremes official doc).\n",
    "    dictionary_LDA.filter_extremes(no_below=3, no_above=0.6)\n",
    "\n",
    "    # Converts each document into a list of BoW (list of (id_term, term_frequency) for each term in doc)\n",
    "    corpus_idbow_freq = [dictionary_LDA.doc2bow(document_words) for document_words in documents_words]\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "    lda_model = models.LdaModel(corpus_idbow_freq, num_topics=topic_num, \\\n",
    "                                id2word=dictionary_LDA, \\\n",
    "                                passes=3, alpha=[0.01] * topic_num, \\\n",
    "                                eta=[0.01] * len(dictionary_LDA.keys()))\n",
    "    \n",
    "    return lda_model, corpus_idbow_freq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115948b8-8e87-43b2-8748-d51d60740723",
   "metadata": {},
   "source": [
    "Show topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657c372c-ce95-44c9-b774-358bd18a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:  100\n",
      "Topic 0 : [('clause', 0.045), ('example', 0.02), ('third', 0.02), ('perfect', 0.016), ('conditionals', 0.014)]\n",
      "Topic 1 : [('travel', 0.017), ('money', 0.014), ('lot', 0.011), ('holiday', 0.011), ('dream', 0.01)]\n",
      "Topic 2 : [('clause', 0.026), ('situation', 0.015), ('condition', 0.015), ('result', 0.015), ('happen', 0.015)]\n",
      "Topic 3 : [('students', 0.065), ('exam', 0.035), ('book', 0.026), ('sb', 0.026), ('speak', 0.025)]\n",
      "Topic 4 : [('word', 0.023), ('clause', 0.021), ('happen', 0.014), ('example', 0.013), ('noun', 0.013)]\n",
      "Topic 5 : [('level', 0.02), ('teach', 0.016), ('video', 0.015), ('love', 0.015), ('esl', 0.015)]\n",
      "Topic 6 : [('hotel', 0.038), ('book', 0.035), ('holiday', 0.025), ('beach', 0.023), ('travel', 0.016)]\n",
      "Topic 7 : [('lesson', 0.038), ('student', 0.012), ('grammar', 0.01), ('learn', 0.01), ('language', 0.009)]\n",
      "Topic 8 : [('clause', 0.015), ('condition', 0.013), ('happen', 0.012), ('result', 0.012), ('book', 0.01)]\n",
      "Topic 9 : [('travel', 0.031), ('article', 0.026), ('word', 0.021), ('journey', 0.02), ('example', 0.017)]\n"
     ]
    }
   ],
   "source": [
    "documents_words = read_corpus(\"travelling.txt\")\n",
    "\n",
    "model, corpus_idbow_freq = topic_modelling(documents_words)\n",
    "\n",
    "topics = {'Topic ' + str(i): [(token, round(score, 3)) for token, score in model.show_topic(i, topn=topic_words_num)] for i in range(0, model.num_topics)}\n",
    "for key, value in topics.items():\n",
    "    print(key,\":\", topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483113ca-cc75-42b0-989b-db18a485ea1a",
   "metadata": {},
   "source": [
    "Show topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3148513-5772-4920-b54c-322c1335b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents topic list\n",
      "Doc 0 : [(1, 0.8516115), (9, 0.14705722)]\n",
      "Doc 1 : [(1, 0.1281324), (3, 0.2067905), (6, 0.6648527)]\n",
      "Doc 2 : [(1, 0.43143946), (7, 0.5667039)]\n",
      "Doc 3 : [(0, 0.7186929), (3, 0.101707526), (7, 0.05541384), (9, 0.12384491)]\n",
      "Doc 4 : [(7, 0.99982)]\n",
      "Doc 5 : [(3, 0.08431996), (4, 0.04471661), (7, 0.46974307), (9, 0.40117958)]\n",
      "Doc 6 : [(7, 0.99901193)]\n",
      "Doc 7 : [(0, 0.7027754), (2, 0.11124558), (7, 0.18546453)]\n",
      "Doc 8 : [(1, 0.99991614)]\n",
      "Doc 9 : [(1, 0.999696)]\n",
      "Doc 10 : [(2, 0.73135585), (4, 0.26853248)]\n",
      "Doc 11 : [(4, 0.99972486)]\n",
      "Doc 12 : [(8, 0.9991354)]\n",
      "Doc 13 : [(5, 0.09053723), (7, 0.9091687)]\n",
      "Doc 14 : [(5, 0.7187402), (7, 0.25519317), (9, 0.02567997)]\n",
      "Doc 15 : [(1, 0.9998529)]\n",
      "Doc 16 : [(2, 0.15852156), (4, 0.3549142), (8, 0.2116178), (9, 0.27447054)]\n",
      "Doc 17 : [(1, 0.9090508), (4, 0.0344841), (5, 0.016202021), (9, 0.040096387)]\n",
      "Doc 18 : [(2, 0.84581715), (4, 0.15388443)]\n",
      "Doc 19 : [(0, 0.2047706), (2, 0.20065314), (4, 0.55849), (7, 0.035849158)]\n",
      "Doc 20 : [(2, 0.26951432), (4, 0.14435573), (7, 0.20139407), (8, 0.384618)]\n",
      "Doc 21 : [(0, 0.22680461), (2, 0.5160523), (4, 0.21951692), (7, 0.037392765)]\n",
      "Doc 22 : [(0, 0.9996566)]\n",
      "Doc 23 : [(2, 0.99968976)]\n",
      "Doc 24 : [(0, 0.9990227)]\n",
      "Doc 25 : [(2, 0.9998448)]\n",
      "Doc 26 : [(0, 0.1196647), (1, 0.048289225), (5, 0.0117667895), (7, 0.084207855), (8, 0.73589903)]\n",
      "Doc 27 : [(4, 0.5898479), (7, 0.40870005)]\n",
      "Doc 28 : [(2, 0.9997369)]\n",
      "Doc 29 : [(2, 0.99973994)]\n",
      "Doc 30 : [(2, 0.27570403), (6, 0.36625892), (7, 0.35764834)]\n",
      "Doc 31 : [(9, 0.9994975)]\n",
      "Doc 32 : [(9, 0.9994975)]\n",
      "Doc 33 : [(7, 0.9999025)]\n",
      "Doc 34 : [(9, 0.9998308)]\n",
      "Doc 35 : [(2, 0.06420267), (4, 0.37090054), (7, 0.2098851), (8, 0.35475197)]\n",
      "Doc 36 : [(5, 0.9996387)]\n",
      "Doc 37 : [(1, 0.24700296), (2, 0.12407568), (7, 0.62867755)]\n",
      "Doc 38 : [(2, 0.21780916), (7, 0.7810494)]\n",
      "Doc 39 : [(2, 0.06736935), (3, 0.01907612), (7, 0.9134496)]\n",
      "Doc 40 : [(1, 0.066263184), (6, 0.9332038)]\n",
      "Doc 41 : [(0, 0.45576265), (4, 0.5431996)]\n",
      "Doc 42 : [(2, 0.014140143), (6, 0.16204719), (7, 0.82371885)]\n",
      "Doc 43 : [(0, 0.13761768), (4, 0.77328825), (7, 0.08491018)]\n",
      "Doc 44 : [(0, 0.023630477), (1, 0.23352027), (2, 0.2836476), (5, 0.034240752), (7, 0.40419334), (9, 0.020553757)]\n",
      "Doc 45 : [(5, 0.9996236)]\n",
      "Doc 46 : [(7, 0.99984425)]\n",
      "Doc 47 : [(3, 0.99998856)]\n",
      "Doc 48 : [(4, 0.5046114), (7, 0.49521282)]\n",
      "Doc 49 : [(0, 0.053337984), (2, 0.633847), (7, 0.11089439), (8, 0.20156775)]\n",
      "Doc 50 : [(6, 0.9999494)]\n",
      "Doc 51 : [(1, 0.08758319), (6, 0.5888864), (7, 0.3233765)]\n",
      "Doc 52 : [(0, 0.036389947), (2, 0.0862275), (7, 0.87715894)]\n",
      "Doc 53 : [(0, 0.07635352), (1, 0.10321993), (2, 0.019841932), (4, 0.017378874), (7, 0.7831148)]\n",
      "Doc 54 : [(7, 0.99894226)]\n",
      "Doc 55 : [(9, 0.99911845)]\n",
      "Doc 56 : [(1, 0.03432092), (5, 0.2702924), (9, 0.6949702)]\n",
      "Doc 57 : [(2, 0.99876875)]\n",
      "Doc 58 : [(2, 0.82453746), (5, 0.031047467), (7, 0.14422381)]\n",
      "Doc 59 : [(5, 0.99972147)]\n",
      "Doc 60 : [(2, 0.22055306), (7, 0.7789847)]\n",
      "Doc 61 : [(4, 0.9987061)]\n",
      "Doc 62 : [(1, 0.9993435)]\n",
      "Doc 63 : [(1, 0.53533), (7, 0.26721466), (9, 0.19713423)]\n",
      "Doc 64 : [(0, 0.06807011), (5, 0.8329104), (7, 0.09881834)]\n",
      "Doc 65 : [(4, 0.20507823), (9, 0.7924292)]\n",
      "Doc 66 : [(9, 0.9987854)]\n",
      "Doc 67 : [(9, 0.99852693)]\n",
      "Doc 68 : [(9, 0.9987854)]\n",
      "Doc 69 : [(5, 0.99960715)]\n",
      "Doc 70 : [(0, 0.48522073), (1, 0.40816355), (4, 0.08507964), (7, 0.020991027)]\n",
      "Doc 71 : [(1, 0.7738049), (7, 0.22573812)]\n",
      "Doc 72 : [(2, 0.9919725)]\n",
      "Doc 73 : [(1, 0.8581369), (2, 0.14095488)]\n",
      "Doc 74 : [(1, 0.38302833), (4, 0.61586195)]\n",
      "Doc 75 : [(7, 0.99981964)]\n",
      "Doc 76 : [(1, 0.95445675), (4, 0.04539648)]\n",
      "Doc 77 : [(3, 0.02302167), (7, 0.9767169)]\n",
      "Doc 78 : [(4, 0.9993881)]\n",
      "Doc 79 : [(1, 0.14291468), (4, 0.8163877), (8, 0.020156031), (9, 0.020427773)]\n",
      "Doc 80 : [(1, 0.08488337), (5, 0.914831)]\n",
      "Doc 81 : [(6, 0.03266932), (7, 0.29430488), (9, 0.67281926)]\n",
      "Doc 82 : [(6, 0.039781116), (7, 0.9593949)]\n",
      "Doc 83 : [(1, 0.9985959)]\n",
      "Doc 84 : [(6, 0.12104862), (7, 0.03374809), (8, 0.84504014)]\n",
      "Doc 85 : [(2, 0.998734)]\n",
      "Doc 86 : [(0, 0.9816092), (2, 0.018103072)]\n",
      "Doc 87 : [(0, 0.9996267)]\n",
      "Doc 88 : [(2, 0.3561897), (9, 0.6426179)]\n",
      "Doc 89 : [(9, 0.9995835)]\n",
      "Doc 90 : [(2, 0.1985635), (7, 0.20079707), (9, 0.5993207)]\n",
      "Doc 91 : [(5, 0.9972808)]\n",
      "Doc 92 : [(4, 0.36411485), (8, 0.6352134)]\n",
      "Doc 93 : [(4, 0.9998171)]\n",
      "Doc 94 : [(8, 0.9989169)]\n",
      "Doc 95 : [(7, 0.99890304)]\n",
      "Doc 96 : [(0, 0.044948068), (4, 0.34977284), (7, 0.6049762)]\n",
      "Doc 97 : [(7, 0.74585956), (9, 0.2533078)]\n",
      "Doc 98 : [(0, 0.9996553)]\n",
      "Doc 99 : [(4, 0.27565375), (7, 0.7237949)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Documents topic list\")\n",
    "for i in range (0, len(corpus_idbow_freq)):\n",
    "    print (\"Doc\", i, \":\", model[corpus_idbow_freq[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b1a5a-a3e6-41b6-a63a-d687fac108e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
