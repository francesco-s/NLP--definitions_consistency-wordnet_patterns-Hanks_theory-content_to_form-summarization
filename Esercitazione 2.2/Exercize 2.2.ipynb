{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3db477-5aaa-4665-837e-b7e500236e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aa620f-72b9-42ab-8ed7-30364228ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    return remove_stopwords(tokenize_sentence(remove_punctuation(sentence)))\n",
    "\n",
    "# Remove punctuation from a list of words\n",
    "def remove_punctuation(sentence):\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\n",
    "# Remove stopwords from a list of words\n",
    "def remove_stopwords(words_list):\n",
    "    stopwords = open(\"stop_words_FULL.txt\", \"r\")\n",
    "    stopwords_list = []\n",
    "    for word in stopwords:\n",
    "        stopwords_list.append(word.replace('\\n', ''))\n",
    "    stopwords.close()\n",
    "    return [value.lower() for value in words_list if value.lower() not in stopwords_list]\n",
    "\n",
    "# Tokenize the input sentence and also lemmatize its words\n",
    "def tokenize_sentence(sentence):\n",
    "    words_list = []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    for tag in nltk.pos_tag(word_tokenize(sentence)):\n",
    "        if (tag[1][:2] == \"NN\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.NOUN))\n",
    "        elif (tag[1][:2] == \"VB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.VERB))\n",
    "        elif (tag[1][:2] == \"RB\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADV))\n",
    "        elif (tag[1][:2] == \"JJ\"):\n",
    "            words_list.append(lmtzr.lemmatize(tag[0], pos=wn.ADJ))\n",
    "    return words_list\n",
    "\n",
    "# Union of the pre-processed words of the definitions and terms from the examples in WN for a sense.\n",
    "def get_signature(sense):\n",
    "    signature = []\n",
    "    for word in tokenize_sentence(sense.definition()):  # definition tokenization\n",
    "        signature.append(word)\n",
    "    for example in sense.examples():  # example tokenization\n",
    "        for word in tokenize_sentence(example):\n",
    "            # Merge definition and examples\n",
    "            signature.append(word)\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925cc97-00f4-44de-bbdf-20c5ebee3e01",
   "metadata": {},
   "source": [
    "Read corpus file. It return a list of list of documents and words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4d1d86-387b-4b38-a1d8-9e12a3c75aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(txt_file):\n",
    "    \n",
    "    with open(txt_file, encoding='utf-8') as file:\n",
    "        # for each doc create list of pre-processed words in that doc (list of lists)\n",
    "        documents_words = []\n",
    "\n",
    "        for line in file:\n",
    "            if \"<doc\" in line:  # tag for new doc\n",
    "                document_words = []  # list of words that will be part of the document\n",
    "                while True:\n",
    "                    next_line = file.readline()  # read next line\n",
    "\n",
    "                    # remove unuseful tags\n",
    "                    next_line_proc = next_line.replace(\"<p> \", \"\").replace(\"</p>\\n\", \"\").replace(\"/p\", \"\")\n",
    "\n",
    "                    if \"</doc>\" in next_line:\n",
    "                        break\n",
    "                    # pre-processing steps\n",
    "                    sentence_words = pre_processing(next_line_proc)\n",
    "                    document_words.extend(sentence_words)\n",
    "                documents_words.append(document_words)\n",
    "        file.close()\n",
    "\n",
    "    print(\"Documents number: \", len(documents_words))\n",
    "    \n",
    "    return documents_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dced5e-4328-426d-a7a4-c0043841b2cb",
   "metadata": {},
   "source": [
    "Topic modelling using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732815ad-9ac4-4efc-ba4c-452675e0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "topic_num = 10\n",
    "topic_words_num = 5\n",
    "\n",
    "def topic_modelling(documents_words):\n",
    "    \n",
    "    # Create a dict with integer keys for all words\n",
    "    dictionary_LDA = corpora.Dictionary(documents_words)\n",
    "\n",
    "    # delete all terms that do NOT appear in at least 3 documents.\n",
    "    # delete all terms that appear in more than 60% of documents (see filter_extremes official doc).\n",
    "    dictionary_LDA.filter_extremes(no_below=3, no_above=0.6)\n",
    "\n",
    "    # Converts each document into a list of BoW (list of (id_term, term_frequency) for each term in doc)\n",
    "    corpus_idbow_freq = [dictionary_LDA.doc2bow(document_words) for document_words in documents_words]\n",
    "    \n",
    "    # https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "    lda_model = models.LdaModel(corpus_idbow_freq, num_topics=topic_num, \\\n",
    "                                id2word=dictionary_LDA, \\\n",
    "                                passes=3, alpha=[0.01] * topic_num, \\\n",
    "                                eta=[0.01] * len(dictionary_LDA.keys()))\n",
    "    \n",
    "    return lda_model, corpus_idbow_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115948b8-8e87-43b2-8748-d51d60740723",
   "metadata": {},
   "source": [
    "Show topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657c372c-ce95-44c9-b774-358bd18a8eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents number:  100\n",
      "Topic 0 : [('book', 0.024), ('hotel', 0.022), ('beach', 0.019), ('holiday', 0.015), ('travel', 0.011)]\n",
      "Topic 1 : [('travel', 0.022), ('hotel', 0.02), ('place', 0.015), ('money', 0.014), ('stay', 0.012)]\n",
      "Topic 2 : [('learn', 0.02), ('word', 0.019), ('language', 0.019), ('student', 0.015), ('article', 0.012)]\n",
      "Topic 3 : [('__________________', 0.027), ('students', 0.026), ('function', 0.015), ('exam', 0.011), ('activity', 0.01)]\n",
      "Topic 4 : [('clause', 0.029), ('happen', 0.013), ('situation', 0.011), ('conditionals', 0.011), ('dont', 0.01)]\n",
      "Topic 5 : [('lesson', 0.014), ('speak', 0.013), ('clause', 0.013), ('students', 0.012), ('audio', 0.01)]\n",
      "Topic 6 : [('clause', 0.022), ('word', 0.019), ('noun', 0.013), ('tense', 0.011), ('dailystep', 0.011)]\n",
      "Topic 7 : [('students', 0.063), ('exam', 0.033), ('book', 0.028), ('sb', 0.025), ('speak', 0.025)]\n",
      "Topic 8 : [('clause', 0.033), ('result', 0.027), ('example', 0.027), ('conditionals', 0.02), ('third', 0.02)]\n",
      "Topic 9 : [('lesson', 0.087), ('website', 0.018), ('clause', 0.013), ('video', 0.013), ('activity', 0.012)]\n"
     ]
    }
   ],
   "source": [
    "documents_words = read_corpus(\"travelling.txt\")\n",
    "\n",
    "model, corpus_idbow_freq = topic_modelling(documents_words)\n",
    "\n",
    "topics = {'Topic ' + str(i): [(token, round(score, 3)) for token, score in model.show_topic(i, topn=topic_words_num)] for i in range(0, model.num_topics)}\n",
    "for key, value in topics.items():\n",
    "    print(key,\":\", topics[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483113ca-cc75-42b0-989b-db18a485ea1a",
   "metadata": {},
   "source": [
    "Show topics for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3148513-5772-4920-b54c-322c1335b610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents topic list\n",
      "Doc 0 : [(0, 0.2397877), (2, 0.53992265), (6, 0.10147474), (8, 0.1178163)]\n",
      "Doc 1 : [(0, 0.15899965), (1, 0.19565934), (2, 0.1550823), (6, 0.062708415), (7, 0.42739)]\n",
      "Doc 2 : [(6, 0.99791145)]\n",
      "Doc 3 : [(2, 0.61039203), (6, 0.08974457), (7, 0.111245535), (9, 0.18827704)]\n",
      "Doc 4 : [(5, 0.7216771), (6, 0.27816293)]\n",
      "Doc 5 : [(2, 0.99993885)]\n",
      "Doc 6 : [(8, 0.92606527), (9, 0.07305638)]\n",
      "Doc 7 : [(2, 0.42251316), (4, 0.22539023), (9, 0.3515822)]\n",
      "Doc 8 : [(1, 0.4828723), (2, 0.517053)]\n",
      "Doc 9 : [(1, 0.44506282), (2, 0.55466694)]\n",
      "Doc 10 : [(8, 0.9998743)]\n",
      "Doc 11 : [(4, 0.95521915), (8, 0.044536207)]\n",
      "Doc 12 : [(5, 0.99913543)]\n",
      "Doc 13 : [(9, 0.9996692)]\n",
      "Doc 14 : [(6, 0.6021036), (7, 0.21609908), (9, 0.18141067)]\n",
      "Doc 15 : [(2, 0.23615946), (3, 0.7275523), (8, 0.01815119), (9, 0.018039074)]\n",
      "Doc 16 : [(8, 0.99928623)]\n",
      "Doc 17 : [(1, 0.21587253), (2, 0.09368496), (6, 0.69024813)]\n",
      "Doc 18 : [(4, 0.7258934), (6, 0.27380815)]\n",
      "Doc 19 : [(4, 0.91771317), (8, 0.08197072)]\n",
      "Doc 20 : [(1, 0.3649999), (4, 0.63484293)]\n",
      "Doc 21 : [(4, 0.8673945), (8, 0.13229431)]\n",
      "Doc 22 : [(8, 0.9996566)]\n",
      "Doc 23 : [(4, 0.99968976)]\n",
      "Doc 24 : [(3, 0.9990228)]\n",
      "Doc 25 : [(4, 0.9998448)]\n",
      "Doc 26 : [(1, 0.40108407), (4, 0.58658135), (5, 0.012093244)]\n",
      "Doc 27 : [(4, 0.19595364), (6, 0.33180362), (8, 0.47097218)]\n",
      "Doc 28 : [(4, 0.9997369)]\n",
      "Doc 29 : [(4, 0.99973994)]\n",
      "Doc 30 : [(6, 0.9995003)]\n",
      "Doc 31 : [(1, 0.1957683), (2, 0.25688386), (7, 0.54695696)]\n",
      "Doc 32 : [(1, 0.19570802), (2, 0.25695902), (7, 0.546942)]\n",
      "Doc 33 : [(9, 0.9999025)]\n",
      "Doc 34 : [(2, 0.9998308)]\n",
      "Doc 35 : [(4, 0.63256514), (6, 0.36708862)]\n",
      "Doc 36 : [(0, 0.9996387)]\n",
      "Doc 37 : [(2, 0.16736278), (4, 0.83235854)]\n",
      "Doc 38 : [(8, 0.998716)]\n",
      "Doc 39 : [(2, 0.99986506)]\n",
      "Doc 40 : [(1, 0.5794878), (2, 0.16791247), (4, 0.18697317), (7, 0.065226644)]\n",
      "Doc 41 : [(0, 0.37585557), (4, 0.62310666)]\n",
      "Doc 42 : [(1, 0.87960476), (2, 0.12028796)]\n",
      "Doc 43 : [(4, 0.99948305)]\n",
      "Doc 44 : [(1, 0.6421495), (4, 0.35742286)]\n",
      "Doc 45 : [(0, 0.99962354)]\n",
      "Doc 46 : [(2, 0.9790171), (9, 0.020844497)]\n",
      "Doc 47 : [(7, 0.99998856)]\n",
      "Doc 48 : [(2, 0.84823906), (6, 0.15158513)]\n",
      "Doc 49 : [(4, 0.99983424)]\n",
      "Doc 50 : [(0, 0.9984782)]\n",
      "Doc 51 : [(1, 0.99980223)]\n",
      "Doc 52 : [(0, 0.81363577), (4, 0.18610868)]\n",
      "Doc 53 : [(2, 0.96240735), (4, 0.037447173)]\n",
      "Doc 54 : [(2, 0.957974), (3, 0.04108572)]\n",
      "Doc 55 : [(0, 0.7341272), (2, 0.26508906)]\n",
      "Doc 56 : [(0, 0.38006154), (1, 0.6194624)]\n",
      "Doc 57 : [(0, 0.20483156), (2, 0.16786118), (4, 0.5340425), (9, 0.092443846)]\n",
      "Doc 58 : [(2, 0.58901864), (9, 0.4107628)]\n",
      "Doc 59 : [(4, 0.9174162), (9, 0.078680456)]\n",
      "Doc 60 : [(8, 0.85786843), (9, 0.14166926)]\n",
      "Doc 61 : [(6, 0.9999404)]\n",
      "Doc 62 : [(1, 0.058147732), (8, 0.94126874)]\n",
      "Doc 63 : [(1, 0.120644316), (2, 0.87898874)]\n",
      "Doc 64 : [(2, 0.10657709), (4, 0.75434357), (9, 0.13887817)]\n",
      "Doc 65 : [(6, 0.99719596)]\n",
      "Doc 66 : [(1, 0.3557253), (7, 0.6431948)]\n",
      "Doc 67 : [(1, 0.29915464), (7, 0.69953567)]\n",
      "Doc 68 : [(1, 0.35572824), (7, 0.6431918)]\n",
      "Doc 69 : [(4, 0.99960715)]\n",
      "Doc 70 : [(2, 0.43739572), (4, 0.56187755)]\n",
      "Doc 71 : [(2, 0.8829721), (4, 0.11657094)]\n",
      "Doc 72 : [(8, 0.9996328)]\n",
      "Doc 73 : [(4, 0.99897826)]\n",
      "Doc 74 : [(4, 0.9987516)]\n",
      "Doc 75 : [(5, 0.7284531), (6, 0.2713866)]\n",
      "Doc 76 : [(4, 0.99983484)]\n",
      "Doc 77 : [(2, 0.97542405), (9, 0.02431447)]\n",
      "Doc 78 : [(4, 0.9993881)]\n",
      "Doc 79 : [(2, 0.04409963), (4, 0.63018423), (6, 0.32558328)]\n",
      "Doc 80 : [(1, 0.020583527), (2, 0.9791308)]\n",
      "Doc 81 : [(0, 0.03971836), (1, 0.68028903), (2, 0.24979743), (9, 0.03001821)]\n",
      "Doc 82 : [(2, 0.999073)]\n",
      "Doc 83 : [(1, 0.9985959)]\n",
      "Doc 84 : [(1, 0.84706235), (2, 0.042313673), (4, 0.063901044), (7, 0.046583094)]\n",
      "Doc 85 : [(9, 0.9987341)]\n",
      "Doc 86 : [(8, 0.89385253), (9, 0.098013766)]\n",
      "Doc 87 : [(8, 0.9996267)]\n",
      "Doc 88 : [(9, 0.9986586)]\n",
      "Doc 89 : [(1, 0.18400458), (4, 0.12287331), (9, 0.69279814)]\n",
      "Doc 90 : [(9, 0.99830484)]\n",
      "Doc 91 : [(0, 0.99728066)]\n",
      "Doc 92 : [(8, 0.9992443)]\n",
      "Doc 93 : [(4, 0.9998171)]\n",
      "Doc 94 : [(4, 0.3343038), (6, 0.5685271), (8, 0.09632661)]\n",
      "Doc 95 : [(6, 0.8853658), (9, 0.11429242)]\n",
      "Doc 96 : [(4, 0.08675011), (8, 0.062011175), (9, 0.8509358)]\n",
      "Doc 97 : [(9, 0.99906343)]\n",
      "Doc 98 : [(8, 0.99965525)]\n",
      "Doc 99 : [(4, 0.9993797)]\n"
     ]
    }
   ],
   "source": [
    "print (\"Documents topic list\")\n",
    "for i in range (0, len(corpus_idbow_freq)):\n",
    "    print (\"Doc\", i, \":\", model[corpus_idbow_freq[i]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "687777cbfed9bcee7b08ad56f15c17d8908988aca6cd0ae5484f3a9e4851bb70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
